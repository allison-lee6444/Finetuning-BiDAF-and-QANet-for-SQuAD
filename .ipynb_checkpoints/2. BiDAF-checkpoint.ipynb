{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVoMc-GrXFj3"
      },
      "source": [
        "## BiDAF\n",
        "\n",
        "This notebook implements one of the most important papers in NLP literature: [BiDAF](https://arxiv.org/abs/1611.01603) or Bidirectional Attention Flow for Machine Comprehension. The key issue that this paper tries to address is that of *early summarization* in all the earlier approches that use attention mechanisms. The attention mechanisms until then were used to obtain a fixed-size summarization of given values and query. This, according to the authors leads to early summarization and loss of information. Moreover, previously, attention was only calculated in only one direction. To improve upon these issues, the authors propose a *hierarchical, multi-stage network*.   \n",
        "> *Our attention layer is not used to summarize the context paragraph into a ﬁxed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to ﬂow through to the subsequent modeling layer. *\n",
        "\n",
        "Let's get into the intricacies of the model.\n",
        "The flow of this notebook will be similar to the previous one.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7nRo-JUXFj6"
      },
      "source": [
        "I use this as my reference. DO NOT RUN!!!\n",
        "\n",
        "DROPOUT_RATE=0.2 # ANCHOR default: 0.2\n",
        "BATCH_SIZE=8 # original is 16. this causes memory overflow\n",
        "CHARACTER_CHANNEL_WIDTH=5 # default 5\n",
        "HIDDEN_SIZE=100 # default 100\n",
        "LEARNING_RATE = 0.01 # default: 0.01 in SGD, 1 in AdaDelta, 1e-3 in Adam\n",
        "OPTIMIZER=\"AdaDelta\"\n",
        "model_name=None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBSRdaZSXFj6"
      },
      "source": [
        "First, we will test parameters: learning rate and optimizer individually as they are relatively isolated from other variables but very close to each other. The state dictionary, loss, EM, F1 can be found under [model_name].pth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE=16"
      ],
      "metadata": {
        "id": "6ZzpLzAKQS7D"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "1haUspYrXFj7"
      },
      "outputs": [],
      "source": [
        "optim_configuration=6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Rj1hyvJUXFj8"
      },
      "outputs": [],
      "source": [
        "optimizer_config_list=[]\n",
        "# configuration 0\n",
        "optimizer_config_list.append((\"AdaDelta\",1,\"AdaDelta_default\"))\n",
        "# configuration 1\n",
        "optimizer_config_list.append((\"AdaDelta\",2,\"AdaDelta_doubled\"))\n",
        "# configuration 2\n",
        "optimizer_config_list.append((\"AdaDelta\",.5,\"AdaDelta_halved\"))\n",
        "\n",
        "# configuration 3\n",
        "optimizer_config_list.append((\"Adam\",.001,\"Adam_default\"))\n",
        "# configuration 4\n",
        "optimizer_config_list.append((\"Adam\",.002,\"Adam_doubled\"))\n",
        "# configuration 5\n",
        "optimizer_config_list.append((\"Adam\",.0005,\"Adam_halved\"))\n",
        "\n",
        "# configuration 6\n",
        "optimizer_config_list.append((\"SGD\",.01,\"SGD_default\"))\n",
        "# configuration 7\n",
        "optimizer_config_list.append((\"SGD\",.02,\"SGD_doubled\"))\n",
        "# configuration 8\n",
        "optimizer_config_list.append((\"SGD\",.005,\"SGD_halved\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "j-4U22HtXFj9"
      },
      "outputs": [],
      "source": [
        "OPTIMIZER, LEARNING_RATE, model_name = optimizer_config_list[optim_configuration]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jPx0RmSXFj-"
      },
      "source": [
        "Now, we have determined that configuration X that has optimizer X with learning rate of X works best for this model with a F1 score of X and EM score of X after 5 epochs. We will then move onto batch size, which is also very much separated from other parameters.\n",
        "\n",
        "Note: model_name will be overwritten here. Please assume the optimizer and learning rate picked is the configuration X with the best overall performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Rs7zzwqvPaR3"
      },
      "outputs": [],
      "source": [
        "dropout_rate_configuration=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_config_list=[]\n",
        "# configuration 0\n",
        "dropout_config_list.append(.2)\n",
        "# configuration 1\n",
        "dropout_config_list.append(0.15)\n",
        "# configuration 2\n",
        "dropout_config_list.append(0.25)"
      ],
      "metadata": {
        "id": "iyfMRcQFE-Oq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DROPOUT_RATE=dropout_config_list[dropout_rate_configuration]"
      ],
      "metadata": {
        "id": "YdOtV9OaF3QO"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "styHP6ZpHKQG"
      },
      "outputs": [],
      "source": [
        "char_width_configuration=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_width_config_list=[]\n",
        "# configuration 0\n",
        "char_width_config_list.append(5)\n",
        "# configuration 1\n",
        "char_width_config_list.append(4)\n",
        "# configuration 2\n",
        "char_width_config_list.append(6)"
      ],
      "metadata": {
        "id": "8irwTXGsHKQH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHARACTER_CHANNEL_WIDTH=char_width_config_list[char_width_configuration]"
      ],
      "metadata": {
        "id": "QvsNjqH6HKQH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LL8Pe8xvHK8u"
      },
      "outputs": [],
      "source": [
        "hidden_configuration=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_config_list=[]\n",
        "# configuration 0\n",
        "hidden_config_list.append(100)\n",
        "# configuration 1\n",
        "hidden_config_list.append(80)\n",
        "# configuration 2\n",
        "hidden_config_list.append(120)"
      ],
      "metadata": {
        "id": "SyA4Huc8HK8v"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HIDDEN_SIZE=hidden_config_list[hidden_configuration]"
      ],
      "metadata": {
        "id": "0nsduYlkHK8v"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pwxVcVxtXFj-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Iuuk4pXFj_",
        "outputId": "a0bdc108-51c8-47f2-81dc-ed7e2977d745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.8/dist-packages (3.4.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Ksi8JbYgXFkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e226928b-20d8-48aa-92f9-6d9c6898f34c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle, time\n",
        "import re, os, string, typing, gc, json\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "nlp = spacy.blank('en')\n",
        "from preprocess import *\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WzJf3DhXFkA"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5o5KXZFXFkA",
        "outputId": "d32246fe-f8af-4183-dbd0-b7035a2dc3bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of data:  442\n",
            "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
            "Title:  University_of_Notre_Dame\n",
            "Length of data:  48\n",
            "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
            "Title:  Super_Bowl_50\n",
            "--------------------------\n",
            "Train list len:  87599\n",
            "Valid list len:  34726\n"
          ]
        }
      ],
      "source": [
        "# load dataset json files\n",
        "\n",
        "train_data = load_json('./data/squad_train.json')\n",
        "valid_data = load_json('./data/squad_dev.json')\n",
        "\n",
        "# parse the json structure to return the data as a list of dictionaries\n",
        "\n",
        "train_list = parse_data(train_data)\n",
        "valid_list = parse_data(valid_data)\n",
        "print('--------------------------')\n",
        "\n",
        "print('Train list len: ',len(train_list))\n",
        "print('Valid list len: ',len(valid_list))\n",
        "\n",
        "# converting the lists into dataframes\n",
        "\n",
        "train_df = pd.DataFrame(train_list)\n",
        "valid_df = pd.DataFrame(valid_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "R7n2FsPsXFkB",
        "outputId": "4f9f0808-755e-4b40-dc75-945807f8e7cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                         id  \\\n",
              "0  5733be284776f41900661182   \n",
              "1  5733be284776f4190066117f   \n",
              "2  5733be284776f41900661180   \n",
              "3  5733be284776f41900661181   \n",
              "4  5733be284776f4190066117e   \n",
              "\n",
              "                                             context  \\\n",
              "0  Architecturally, the school has a Catholic cha...   \n",
              "1  Architecturally, the school has a Catholic cha...   \n",
              "2  Architecturally, the school has a Catholic cha...   \n",
              "3  Architecturally, the school has a Catholic cha...   \n",
              "4  Architecturally, the school has a Catholic cha...   \n",
              "\n",
              "                                            question       label  \\\n",
              "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
              "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
              "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
              "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
              "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
              "\n",
              "                                    answer  \n",
              "0               Saint Bernadette Soubirous  \n",
              "1                a copper statue of Christ  \n",
              "2                        the Main Building  \n",
              "3  a Marian place of prayer and reflection  \n",
              "4       a golden statue of the Virgin Mary  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a86cbb79-7d64-4aae-9eb8-c0950c62a3fa\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>context</th>\n",
              "      <th>question</th>\n",
              "      <th>label</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5733be284776f41900661182</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
              "      <td>[515, 541]</td>\n",
              "      <td>Saint Bernadette Soubirous</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5733be284776f4190066117f</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What is in front of the Notre Dame Main Building?</td>\n",
              "      <td>[188, 213]</td>\n",
              "      <td>a copper statue of Christ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5733be284776f41900661180</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
              "      <td>[279, 296]</td>\n",
              "      <td>the Main Building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5733be284776f41900661181</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What is the Grotto at Notre Dame?</td>\n",
              "      <td>[381, 420]</td>\n",
              "      <td>a Marian place of prayer and reflection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5733be284776f4190066117e</td>\n",
              "      <td>Architecturally, the school has a Catholic cha...</td>\n",
              "      <td>What sits on top of the Main Building at Notre...</td>\n",
              "      <td>[92, 126]</td>\n",
              "      <td>a golden statue of the Virgin Mary</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a86cbb79-7d64-4aae-9eb8-c0950c62a3fa')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a86cbb79-7d64-4aae-9eb8-c0950c62a3fa button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a86cbb79-7d64-4aae-9eb8-c0950c62a3fa');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "mP6rfz8jXFkB"
      },
      "outputs": [],
      "source": [
        "def preprocess_df(df):\n",
        "    \n",
        "    def to_lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    df.context = df.context.apply(to_lower)\n",
        "    df.question = df.question.apply(to_lower)\n",
        "    df.answer = df.answer.apply(to_lower)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "RYTf0QB9XFkB"
      },
      "outputs": [],
      "source": [
        "preprocess_df(train_df)\n",
        "preprocess_df(valid_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ri2CepgcXFkB",
        "outputId": "f39a5269-8e05-4433-d155-5c7df72d2cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 388 ms, sys: 12 ms, total: 400 ms\n",
            "Wall time: 404 ms\n",
            "Number of sentences in dataset:  118822\n"
          ]
        }
      ],
      "source": [
        "# gather text to build vocabularies\n",
        "\n",
        "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
        "print(\"Number of sentences in dataset: \", len(vocab_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNIoNajFXFkB",
        "outputId": "0e1f6db7-c1aa-4861-83a6-37beb3432641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "raw-vocab: 96774\n",
            "vocab-length: 96776\n",
            "word2idx-length: 96776\n",
            "CPU times: user 13.8 s, sys: 303 ms, total: 14.1 s\n",
            "Wall time: 18.3 s\n",
            "----------------------------------\n",
            "raw-char-vocab: 1316\n",
            "char-vocab-intersect: 202\n",
            "char2idx-length: 204\n",
            "CPU times: user 2.26 s, sys: 98.2 ms, total: 2.36 s\n",
            "Wall time: 2.35 s\n"
          ]
        }
      ],
      "source": [
        "# build word and character-level vocabularies\n",
        "\n",
        "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n",
        "print(\"----------------------------------\")\n",
        "%time char2idx, char_vocab = build_char_vocab(vocab_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngBoERTZXFkC",
        "outputId": "7b01b9f7-c2e6-4db9-ca26-2706232b9152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 16.4 s, sys: 43 ms, total: 16.4 s\n",
            "Wall time: 16.5 s\n",
            "CPU times: user 6.48 s, sys: 24.8 ms, total: 6.5 s\n",
            "Wall time: 6.49 s\n",
            "CPU times: user 2.39 s, sys: 7.88 ms, total: 2.4 s\n",
            "Wall time: 2.39 s\n",
            "CPU times: user 891 ms, sys: 2.99 ms, total: 894 ms\n",
            "Wall time: 893 ms\n"
          ]
        }
      ],
      "source": [
        "# numericalize context and questions for training and validation set\n",
        "\n",
        "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
        "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
        "%time train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)\n",
        "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx=word2idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdarlvXDXFkC",
        "outputId": "873e59f1-3ccb-4ac5-a718-44fc42d36d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of error indices: 921\n",
            "Number of error indices: 349\n"
          ]
        }
      ],
      "source": [
        "# get indices with tokenization errors and drop those indices \n",
        "\n",
        "train_err = get_error_indices(train_df, idx2word)\n",
        "valid_err = get_error_indices(valid_df, idx2word)\n",
        "\n",
        "train_df.drop(train_err, inplace=True)\n",
        "valid_df.drop(valid_err, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "OSVhso9eXFkC"
      },
      "outputs": [],
      "source": [
        "# get start and end positions of answers from the context\n",
        "# this is basically the label for training QA models\n",
        "\n",
        "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
        "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
        "\n",
        "train_df['label_idx'] = train_label_idx\n",
        "valid_df['label_idx'] = valid_label_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "4P04dEJhXFkD"
      },
      "outputs": [],
      "source": [
        "# dump to pickle files\n",
        "\n",
        "train_df.to_pickle('bidaftrain.pkl')\n",
        "valid_df.to_pickle('bidafvalid.pkl')\n",
        "\n",
        "with open('bidafw2id.pickle','wb') as handle:\n",
        "    pickle.dump(word2idx, handle)\n",
        "\n",
        "with open('bidafc2id.pickle','wb') as handle:\n",
        "    pickle.dump(char2idx, handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "XGwDCpU4XFkD"
      },
      "outputs": [],
      "source": [
        "# load data from pickle files\n",
        "\n",
        "\n",
        "train_df = pd.read_pickle('bidaftrain.pkl')\n",
        "valid_df = pd.read_pickle('bidafvalid.pkl')\n",
        "\n",
        "with open('bidafw2id.pickle','rb') as handle:\n",
        "    word2idx = pickle.load(handle)\n",
        "with open('bidafc2id.pickle','rb') as handle:\n",
        "    char2idx = pickle.load(handle)\n",
        "\n",
        "idx2word = {v:k for k,v in word2idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCyvgNQRXFkD"
      },
      "source": [
        "## Dataloader/Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "sOG5DK_JXFkD"
      },
      "outputs": [],
      "source": [
        "class SquadDataset:\n",
        "    '''\n",
        "    - Creates batches dynamically by padding to the length of largest example\n",
        "      in a given batch.\n",
        "    - Calulates character vectors for contexts and question.\n",
        "    - Returns tensors for training.\n",
        "    '''\n",
        "    \n",
        "    def __init__(self, data, batch_size):\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
        "        self.data = data\n",
        "        \n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
        "        \n",
        "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n",
        "        \n",
        "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner',\"lemmatizer\"])):\n",
        "            for j, ch in enumerate(word.text):\n",
        "                char_vec[i][j] = char2idx.get(ch, 0)\n",
        "        \n",
        "        return char_vec    \n",
        "    \n",
        "    def get_span(self, text):\n",
        "        \n",
        "        text = nlp(text, disable=['parser','tagger','ner'])\n",
        "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
        "\n",
        "        return span\n",
        "\n",
        "    def __iter__(self):\n",
        "        '''\n",
        "        Creates batches of data and yields them.\n",
        "        \n",
        "        Each yield comprises of:\n",
        "        :padded_context: padded tensor of contexts for each batch \n",
        "        :padded_question: padded tensor of questions for each batch \n",
        "        :char_ctx & ques_ctx: character-level ids for context and question\n",
        "        :label: start and end index wrt context_ids\n",
        "        :context_text,answer_text: used while validation to calculate metrics\n",
        "        :ids: question_ids for evaluation\n",
        "        \n",
        "        '''\n",
        "        \n",
        "        for batch in self.data:\n",
        "            \n",
        "            spans = []\n",
        "            ctx_text = []\n",
        "            answer_text = []\n",
        "            \n",
        "            for ctx in batch.context:\n",
        "                ctx_text.append(ctx)\n",
        "                spans.append(self.get_span(ctx))\n",
        "            \n",
        "            for ans in batch.answer:\n",
        "                answer_text.append(ans)\n",
        "                \n",
        "            \n",
        "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
        "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
        "            \n",
        "            for i, ctx in enumerate(batch.context_ids):\n",
        "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
        "                \n",
        "            max_word_ctx = 0\n",
        "            for context in batch.context:\n",
        "                for word in nlp(context, disable=['parser','tagger','ner']):\n",
        "                    if len(word.text) > max_word_ctx:\n",
        "                        max_word_ctx = len(word.text)\n",
        "            \n",
        "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
        "            for i, context in enumerate(batch.context):\n",
        "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
        "            \n",
        "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
        "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
        "            \n",
        "            for i, ques in enumerate(batch.question_ids):\n",
        "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
        "                \n",
        "            max_word_ques = 0\n",
        "            for question in batch.question:\n",
        "                for word in nlp(question, disable=['parser','tagger','ner']):\n",
        "                    if len(word.text) > max_word_ques:\n",
        "                        max_word_ques = len(word.text)\n",
        "            \n",
        "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
        "            for i, question in enumerate(batch.question):\n",
        "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
        "            \n",
        "            ids = list(batch.id)  \n",
        "            label = torch.LongTensor(list(batch.label_idx))\n",
        "            \n",
        "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
        "            \n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "15GFX63MXFkF"
      },
      "outputs": [],
      "source": [
        "train_dataset = SquadDataset(train_df, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "SgEJ4TWiXFkF"
      },
      "outputs": [],
      "source": [
        "valid_dataset = SquadDataset(valid_df, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "aCm5cEznXFkF"
      },
      "outputs": [],
      "source": [
        "a = next(iter(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2NM5xbYXFkG"
      },
      "source": [
        "## BiDAF Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2oDfA_mXFkG"
      },
      "source": [
        "## Word Embedding\n",
        "\n",
        "> *Word embedding layer also maps each word to a high-dimensional vector space. We use pre-trained word vectors, GloVe to obtain the ﬁxed word embedding of each word. *\n",
        "\n",
        "This model uses 100-dimensional pre-trained word vectors. The `weights_matrix` obtained below is initialized as an `nn.Embedding`'s weight. This is done in the last module in a function which as follows:\n",
        "\n",
        "```\n",
        "weights_matrix = np.load('bidafglove.npy')\n",
        "num_embeddings, embedding_dim = weights_matrix.shape\n",
        "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "qyQoLShIXFkG"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOXLLsAEXFkH",
        "outputId": "a473158f-96ea-4fb7-a954-7cf5302932d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 04:46:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-12-16 04:46:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-12-16 04:46:39--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2022-12-16 04:49:18 (5.17 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "3jMnxFcxXFkH"
      },
      "outputs": [],
      "source": [
        "def get_glove_dict():\n",
        "    '''\n",
        "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
        "    keys and their respective pretrained word vectors as values.\n",
        "\n",
        "    '''\n",
        "    encoding = 'utf-8'\n",
        "    glove_dict = {}\n",
        "    archive = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "    with archive.open(\"glove.6B.100d.txt\", \"r\") as f:\n",
        "        for line in io.TextIOWrapper(f, encoding):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            glove_dict[word] = vector\n",
        "            \n",
        "    f.close()\n",
        "    \n",
        "    return glove_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "D9Vw40VBXFkH"
      },
      "outputs": [],
      "source": [
        "def get_glove_dict_local_machine(): # assumes zip downloaded\n",
        "    '''\n",
        "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
        "    keys and their respective pretrained word vectors as values.\n",
        "\n",
        "    '''\n",
        "    glove_dict = {}\n",
        "    with open(\"./glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], \"float32\")\n",
        "            glove_dict[word] = vector\n",
        "            \n",
        "    f.close()\n",
        "    \n",
        "    return glove_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "BR3fvfqTXFkH"
      },
      "outputs": [],
      "source": [
        "glove_dict = get_glove_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "9A21kX-qXFkI"
      },
      "outputs": [],
      "source": [
        "def create_weights_matrix(glove_dict):\n",
        "    '''\n",
        "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
        "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
        "    '''\n",
        "    weights_matrix = np.zeros((len(word_vocab), 100))\n",
        "    words_found = 0\n",
        "    for i, word in enumerate(word_vocab):\n",
        "        try:\n",
        "            weights_matrix[i] = glove_dict[word]\n",
        "            words_found += 1\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return weights_matrix, words_found\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1jxyNvqXFkI",
        "outputId": "892d8a4e-e4a3-43e9-f384-5edfd460b3b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words found in the GloVe vocab:  72687\n"
          ]
        }
      ],
      "source": [
        "weights_matrix, words_found = create_weights_matrix(glove_dict)\n",
        "print(\"Words found in the GloVe vocab: \" ,words_found)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "5rbx3XUzXFkI"
      },
      "outputs": [],
      "source": [
        "# dump the weights to load in future\n",
        "\n",
        "np.save('bidafglove_tv.npy', weights_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4rf3V-dXFkI"
      },
      "source": [
        "## Character Embedding\n",
        "\n",
        "A character embedding is calculated for each context and query word. This is done by using convolutions.   \n",
        ">  *It maps each word to a vector space using character-level CNNs.*\n",
        "\n",
        "Using CNNs in NLP was first proposed by Yoon Kim in 2014 in his paper \"Convolutional Neural Networks for Sentence Classification\". This paper tries to use CNNs in NLP as they are used in vision. Most of the state-of-the-art results in CV at that time were achieved by transfer learning from larger models pretrained on ImageNet. In this paper, they train a simple CNN with one layer of convolution on top of pretrained word vectors and hypothesized that these pretrained word vectors could work as a universal feature extractors for various classification tasks. This is analogous to the earlier layers of vision models like VGG and Inception working as generic feature extractors.\n",
        "\n",
        "The intuition is simple over here. Just as convolutional filters learn various features in an image by operating on its pixels, here they'll do so by operating on characters of words. Let's get into the working of this layer.  \n",
        "\n",
        "We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$.\n",
        "Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$.\n",
        "<img src=\"images/charemb1.PNG\" width=\"500\" height=\"300\"/>\n",
        "\n",
        "Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter.   \n",
        "The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. \n",
        "<img src=\"images/charemb2.PNG\" width=\"500\" height=\"400\"/>  \n",
        "\n",
        "The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows:\n",
        "<img src=\"images/charemb3.PNG\" width=\"900\" height=\"700\"/>    \n",
        "The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector.\n",
        "<img src=\"images/charemb4.PNG\" width=\"500\" height=\"300\"/>     \n",
        "\n",
        "The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100.  \n",
        "\n",
        "The implementation of this layer is fairly straightforward.  The input to this layer is of dimension `[batch_size, seq_len, word_len]` where `seq_len` and `word_len` are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension `[batch_size, seq_len, word_len, emb_dim]`.   \n",
        "We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here,\n",
        "\n",
        "<img src=\"images/conv.PNG\" width=\"600\" height=\"500\"/>  \n",
        "\n",
        "If `padding` = [0,0], `kernel_size` (or filter_size) = [$H_{in}$, $w$], `dilation` = [1,1], `stride` = [1,1] (as visible in images above), then,   \n",
        "$H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1.\n",
        "\n",
        "Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel_size = $L_{in}$. The value of $L_{in}$ =  $W_{in}$ - $w$ - 1.\n",
        "\n",
        "<img src=\"images/maxpool.PNG\" width=\"600\" height=\"500\"/>     \n",
        "If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension   `[batch_size, seq_len, output_channels (or 100)]`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "qULa_r5UXFkI"
      },
      "outputs": [],
      "source": [
        "class CharacterEmbeddingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.char_emb_dim = char_emb_dim\n",
        "        \n",
        "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
        "        \n",
        "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
        "                \n",
        "    def forward(self, x):\n",
        "        # x = [bs, seq_len, word_len]\n",
        "        # returns : [batch_size, seq_len, num_output_channels]\n",
        "        # the output can be thought of as another feature embedding of dim 100.\n",
        "        \n",
        "        batch_size = x.shape[0] # try this to see if this solves out of memory error\n",
        "        \n",
        "        x = self.dropout(self.char_embedding(x))\n",
        "        # x = [bs, seq_len, word_len, char_emb_dim]\n",
        "        \n",
        "        # following three operations manipulate x in such a way that\n",
        "        # it closely resembles an image. this format is important before \n",
        "        # we perform convolution on the character embeddings.\n",
        "        \n",
        "        x = x.permute(0,1,3,2)\n",
        "        # x = [bs, seq_len, char_emb_dim, word_len]\n",
        "        \n",
        "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
        "        # x = [bs*seq_len, char_emb_dim, word_len]\n",
        "        \n",
        "        x = x.unsqueeze(1)\n",
        "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n",
        "        \n",
        "        # x is now in a format that can be accepted by a conv layer. \n",
        "        # think of the tensor above in terms of an image of dimension\n",
        "        # (N, C_in, H_in, W_in).\n",
        "        \n",
        "        x = self.relu(self.char_convolution(x))\n",
        "        # x = [bs*seq_len, out_channels, H_out, W_out]\n",
        "        \n",
        "        x = x.squeeze()\n",
        "        # x = [bs*seq_len, out_channels, W_out]\n",
        "                \n",
        "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
        "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n",
        "        \n",
        "        x = x.view(batch_size, -1, x.shape[-1])\n",
        "        # x = [bs, seq_len, out_channels]\n",
        "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n",
        "        \n",
        "        \n",
        "        return x        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_dHCV4NXFkJ"
      },
      "source": [
        "## Highway Networks\n",
        "\n",
        "Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training *deep* networks was still a challenging task owing to problems such as vanishing gradients etc. Quoting the paper,\n",
        "\n",
        ">  *We present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information ﬂow which is inspired by Long Short Term Memory recurrent neural networks. Due to this gating mechanism, a neural network can have paths along which information can ﬂow across several layers without attenuation. We call such paths information highways, and such networks highway networks.* \n",
        "\n",
        "This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to *learn* to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is.   \n",
        "\n",
        "A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is  \n",
        "\n",
        "$$ y = g(W_{H}.x + b_{H})$$\n",
        "where $g$ is a non-linear activation.  \n",
        "For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$).\n",
        "Then,    \n",
        "  \n",
        "$$ y = T(x) . H(x) + x . C(x) $$ \n",
        "> *We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by\n",
        "transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. *\n",
        "\n",
        "$$ y = T(x) . H(x) + x . (1 - T(x)) $$  \n",
        "  \n",
        "$$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$  \n",
        "where $T(x)$ = $\\sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation.  \n",
        "\n",
        "The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use `nn.ModuleList` to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the `flow_layer` is the same as linear transform $H$ discussed above and `gate_layer` is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above.   \n",
        "  \n",
        "The output of this layer for context is $X$ $\\epsilon$ $R^{\\ d \\ X \\ T}$ and for query is $Q$ $\\epsilon$ $R^{\\ d \\ X \\ J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length.  \n",
        "\n",
        "The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "51uYJKaAXFkJ"
      },
      "outputs": [],
      "source": [
        "class HighwayNetwork(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, num_layers=2):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        \n",
        "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
        "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            \n",
        "            flow_value = F.relu(self.flow_layer[i](x))\n",
        "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
        "            \n",
        "            x = gate_value * flow_value + (1-gate_value) * x\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjuEeTMsXFkJ"
      },
      "source": [
        "## Contextual Embedding\n",
        "\n",
        "This layer is the final embedding layer in the model.\n",
        "> *Bi-Directional Attention Flow (BIDAF) network, a hierarchical multi-stage architecture for modeling the representations ofthe context paragraph at different levels of granularity. BIDAF includes character-level, word-level, and contextual embeddings*\n",
        "\n",
        "The output of highway layers is passed to a bidirection LSTM to model the temporal features of the text. This is done for both, the context and the query.   \n",
        ">  *Utilizes contextual cues from surrounding words to reﬁne the embedding of the words*\n",
        "\n",
        "The output of this layer for context is called as $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and for query it is named as $U$ $\\epsilon$ $R^{\\ 2d \\  X \\  J}$. The $2d$ is because of the features from both forward and backward LSTMs. In code, we simply use the outputs of the LSTM layer and ignore the hidden states."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "qSVQlRlZXFkJ"
      },
      "outputs": [],
      "source": [
        "class ContextualEmbeddingLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        self.highway_net = HighwayNetwork(input_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n",
        "        # the input is the concatenation of word and characeter embeddings\n",
        "        # for the sequence.\n",
        "        \n",
        "        highway_out = self.highway_net(x)\n",
        "        # highway_out = [bs, seq_len, input_dim]\n",
        "        \n",
        "        outputs, _ = self.lstm(highway_out)\n",
        "        # outputs = [bs, seq_len, emb_dim*2]\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "b6e0g0WVXFkK"
      },
      "source": [
        "## Attention Flow Layer\n",
        "\n",
        "\n",
        "> *It is worth noting that the ﬁrst three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional neural networks in the computer vision ﬁeld.*\n",
        "\n",
        "The output of the previous layer: contextual representation of context $H$ and query $U$ are passed on to this layer. Until now processing of the context and the query has been independent of each other. This layer, however, is responsible for fusing and linking the context and query representations.   \n",
        "\n",
        "This layer calculates attention in two directions: from context to query and from query to context. Attention vectors for these calculations are derived from a common matrix which is called as the similarity matrix and is denoted by $S$ $\\epsilon$ $R^{\\ T \\ X \\ J}$. $T$ is the length of the context (number of words/tokens) and $J$ is the length of the query for each training example. The similarity matrix is computed by,\n",
        "$$ S_{tj} = \\alpha\\ (H_{:t}, U_{:j}) $$  \n",
        "\n",
        "where $S_{tj}$ $\\epsilon$ $R$.  \n",
        "$S_{tj}$ is a single float value that determines the similarity between the $t$-th context word and $j$-th query word.\n",
        "$\\alpha$ is a trainable function that encodes the similarity between two input vectors $H_{:t}$ and $U_{:j}$.  \n",
        "$H_{:t}$ is the contextual representation of the $t$-th context word and $U_{:j}$ is the contextual representation of the $j$-th query word. \n",
        "<img src=\"images/simimat.PNG\" width=\"500\" height=\"500\"/>     \n",
        "\n",
        "\n",
        "\n",
        "The trainable function is defined as,  \n",
        "\n",
        "$$ \\alpha \\ (h, u) = w_{(S)}^{T}\\ [h\\ ;\\ u \\ ; \\ h \\circ u]  $$\n",
        "where $;$ denotes concatenation and $\\circ$ denotes an element wise product.    \n",
        "$w_{T}^{S}$ is a trainable weight matrix of dimension $6d$. This is because $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and $U$ $\\epsilon$ $R^{\\ 2d \\ X \\ J}$, and we are concatenating 3 such vectors.  \n",
        "\n",
        "In code, all the computations above are performed directly by operating on matrices/ tensors. We first `repeat` the contextual representations of context and query along appropriate dimensions to get two tensors of shape `[batch_size, ctx_len, query_len, emb_dim*2 (d*2)]`. $w_{(S)}^T$ is characterized by a linear layer called `similarity_weight`. We concat the tensors along the last dimension and pass it through the linear layer. \n",
        "\n",
        "### Context-to-Query Attention\n",
        "\n",
        "> *. Context-to-query (C2Q) attention signiﬁes which query words are most relevant to each context word.*\n",
        " \n",
        "Let $a_{t}$ represent the vector that encodes the attention paid on each query word by the $t$-th context word.   \n",
        "$$ \\sum_{j}a_{tj} = 1  $$  \n",
        "$$ a_{t} = softmax(S_{:t}) $$\n",
        "where $a_{t}$ $\\epsilon$ $R^{J}$   \n",
        "This can be visualized as,\n",
        "<img src=\"images/c2q.PNG\" width=\"700\" height=\"750\"/>     \n",
        "\n",
        "Subsequently, each attended query vector is calculated by,\n",
        "$$ \\overline U_{:t} = \\sum_{j} a_{tj} U_{:j} $$\n",
        "\n",
        "This vector indicates the most important word in the query with respect to context.  \n",
        "\n",
        "In code, again vectorizing operations into tensors, this simply involves taking a softmax of the similarity matrix across the last dimension, i.e across the columns and multiplying this with $U$. The shape of similarity matrix is `[batch_size, ctx_len, query_len]`. The shape of $U$ is `[batch_size, query_len, emb_dim*2]` . Hence performing matrix multiplication by using `torch.bmm` would give us a tensor of shape `[batch_size, ctx_len, emb_dim*2 ]`. Therefore, $\\overline U$ will be $2d$ X $T$\n",
        " matrix.\n",
        "\n",
        "\n",
        "### Query-to-Context Attention\n",
        "\n",
        ">  *Query-to-context (Q2C) attention signiﬁes which context words have the closest similarity to one of the query words and are hence critical for answering the query.*\n",
        "\n",
        "The attention vector here is calculated as,  \n",
        "$$ b = softmax \\ (max_{col} \\ (S)) \\ \\epsilon R^{T} $$  \n",
        "where $max_{col}$ performs the maximum function across the column. The attended context vector is then calculated as\n",
        "\n",
        "$$ \\overline h = \\sum_{t} b_{t} H_{:t}$$\n",
        "\n",
        "The above equations can be visualized as,\n",
        "\n",
        "<img src=\"images/q2c.PNG\" width=\"800\" height=\"900\"/>\n",
        "\n",
        "The above figure helps us in understanding that for each training example we'll get a single $T$ - dimensional vector. This vector will then be multiplied by the contextual representation $H$ to get a single $2d$ vector.  To get the matrix $\\overline H$, we need to tile/repeat this vector $T$ times to get a $2d$-by-$T$ representation.\n",
        "This is unlike the previous attention we calculated where we directly got a $2d$-by-$T$ matrix.\n",
        "\n",
        "The implementation for this part is straightforward and very similar to the explanation above. We first perform maximum across columns using `torch.max`. This gives a tensor of shape `[batch_size, ctx_len]`. This is then passed through a softmax to get weights that add up to 1. We then insert an extra dimension in this tensor and multiply it with $H$. This gives a tensor of shape `[batch_size, 1, emb_dim*2]`. This is exactly what we have discussed above. This tensor corresponds to $\\overline h$. We then `repeat` this $T$ times to get $\\overline H$ of shape `[batch_size, ctx_len, emb_dim*2]`\n",
        "\n",
        "\n",
        "### Combining attention vectors and contextual embeddings\n",
        "\n",
        "> *Finally, the contextual embeddings and the attention vectors are combined together to yield G, where each column vector can be considered as the query-aware representation of each context word.*\n",
        "\n",
        "$G$ is defined as,\n",
        "\n",
        "$$ G_{:t} = \\beta \\ (\\ H_{:t}\\ ;\\ \\overline U_{:t} \\ ;\\ \\overline H_{:t}  \\ )$$\n",
        "\n",
        "where $\\beta$ is a trainable function.  \n",
        "$$ \\beta (h, \\overline u, \\overline h) = [h\\ ;\\ \\overline u\\ ;\\ h \\circ \\overline u\\ ;\\ h \\circ \\overline h\\ ] $$\n",
        "\n",
        "This concatenation gives a $8d$ vector. Hence the trainable weight here should have an input dimension of $8d$. However here we don't involve a trainable weight because according to the authors,\n",
        "\n",
        "> *While the β function can be an arbitrary trainable neural network, such as multi-layer perceptron, a simple concatenation as following still shows good performance in our experiments.*\n",
        "\n",
        "In code, this simply involves a single line of concatenating tensors using `torch.cat` to yeild $G$. This tensor holds the query-aware representation of the context words.\n",
        "\n",
        "\n",
        "## Modeling Layer\n",
        "\n",
        "$G$ is then passed on to this layer. This layer is responsible for capturing temporal features interactions among the context words. This is done using a bidirectional LSTM. The difference between this layer and the contextual layer, both of which involve an LSTM layer is that here we have a *query-aware* representation of the context while in the contextual layer, encoding of the context and query was independent.  \n",
        "Using an LSTM layer with a hidden size of $d$ we get a $2d$-by-$T$ output called $M$.  \n",
        "\n",
        "> *Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and the query.*\n",
        "\n",
        "\n",
        "## Output Layer\n",
        "\n",
        "The start index $p^{1}$ of the answer span is calculated by,\n",
        "\n",
        "<img src=\"images/p1.PNG\" width=\"300\" height=\"300\"/>\n",
        "\n",
        "where $w_{(p^{1})}$ is a $10d$ trainable weight vector which corresponds to a linear layer in code called `output_start`.  \n",
        "To predict the end of the answer span, $M$ is once again passed to a bidirectional LSTM to get $M^{2}$ which again is a $2d$-by-$T$ matrix. The end is then computed as,  \n",
        "\n",
        "<img src=\"images/p2.PNG\" width=\"300\" height=\"300\"/>  \n",
        "\n",
        "\n",
        "$w_{(p^{2})}$ is again a $10d$ trainable weight vector and corresponds to the linear layer `output_end`. \n",
        "\n",
        "In code we don't explicitly calculate softmax since this is taken care of while calculating the losses during training. \n",
        "\n",
        "> *The output layer is application-speciﬁc. The modular nature of BIDAF allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. *\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "8pwrHBFiXFkL"
      },
      "outputs": [],
      "source": [
        "class BiDAF(nn.Module):\n",
        "    \n",
        "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
        "                 kernel_size, ctx_hidden_dim, device):\n",
        "        '''\n",
        "        char_vocab_dim = len(char2idx)\n",
        "        emb_dim = 100\n",
        "        char_emb_dim = 8\n",
        "        num_output_chanels = 100\n",
        "        kernel_size = (8,5)\n",
        "        ctx_hidden_dim = 100\n",
        "        '''\n",
        "        super().__init__()\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.word_embedding = self.get_glove_embedding()\n",
        "        \n",
        "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
        "                                                      num_output_channels, kernel_size)\n",
        "        \n",
        "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout()\n",
        "        \n",
        "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
        "        \n",
        "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n",
        "        \n",
        "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
        "        \n",
        "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
        "        \n",
        "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
        "        \n",
        "    \n",
        "    def get_glove_embedding(self):\n",
        "        \n",
        "        weights_matrix = np.load('bidafglove_tv.npy')\n",
        "        num_embeddings, embedding_dim = weights_matrix.shape\n",
        "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
        "\n",
        "        return embedding\n",
        "        \n",
        "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
        "        # ctx = [bs, ctx_len]\n",
        "        # ques = [bs, ques_len]\n",
        "        # char_ctx = [bs, ctx_len, ctx_word_len]\n",
        "        # char_ques = [bs, ques_len, ques_word_len]\n",
        "        \n",
        "        ctx_len = ctx.shape[1]\n",
        "        \n",
        "        ques_len = ques.shape[1]\n",
        "        \n",
        "        ## GET WORD AND CHARACTER EMBEDDINGS\n",
        "        \n",
        "        ctx_word_embed = self.word_embedding(ctx)\n",
        "        # ctx_word_embed = [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        ques_word_embed = self.word_embedding(ques)\n",
        "        # ques_word_embed = [bs, ques_len, emb_dim]\n",
        "        \n",
        "        ctx_char_embed = self.character_embedding(char_ctx)\n",
        "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n",
        "        \n",
        "        ques_char_embed = self.character_embedding(char_ques)\n",
        "        # ques_char_embed = [bs, ques_len, emb_dim]\n",
        "        \n",
        "        ## CREATE CONTEXTUAL EMBEDDING\n",
        "        \n",
        "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
        "        # [bs, ques_len, emb_dim*2]\n",
        "        \n",
        "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
        "        # [bs, ques_len, emb_dim*2]\n",
        "        \n",
        "        \n",
        "        ## CREATE SIMILARITY MATRIX\n",
        "        \n",
        "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
        "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
        "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        elementwise_prod = torch.mul(ctx_, ques_)\n",
        "        # [bs, ctx_len, ques_len, emb_dim*2]\n",
        "        \n",
        "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
        "        # [bs, ctx_len, ques_len, emb_dim*6]\n",
        "        \n",
        "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
        "        # [bs, ctx_len, ques_len]\n",
        "        \n",
        "        \n",
        "        ## CALCULATE CONTEXT2QUERY ATTENTION\n",
        "        \n",
        "        a = F.softmax(similarity_matrix, dim=-1)\n",
        "        # [bs, ctx_len, ques_len]\n",
        "        \n",
        "        c2q = torch.bmm(a, ques_contextual_emb)\n",
        "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        \n",
        "        ## CALCULATE QUERY2CONTEXT ATTENTION\n",
        "        \n",
        "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
        "        # [bs, ctx_len]\n",
        "        \n",
        "        b = b.unsqueeze(1)\n",
        "        # [bs, 1, ctx_len]\n",
        "        \n",
        "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
        "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n",
        "        \n",
        "        q2c = q2c.repeat(1, ctx_len, 1)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ## QUERY AWARE REPRESENTATION\n",
        "        \n",
        "        G = torch.cat([ctx_contextual_emb, c2q, \n",
        "                       torch.mul(ctx_contextual_emb,c2q), \n",
        "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n",
        "        \n",
        "        # [bs, ctx_len, emb_dim*8]\n",
        "        \n",
        "        \n",
        "        ## MODELING LAYER\n",
        "        \n",
        "        M, _ = self.modeling_lstm(G)\n",
        "        # [bs, ctx_len, emb_dim*2]\n",
        "        \n",
        "        ## OUTPUT LAYER\n",
        "        \n",
        "        M2, _ = self.end_lstm(M)\n",
        "        \n",
        "        # START PREDICTION\n",
        "        \n",
        "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
        "        # [bs, ctx_len, 1]\n",
        "        \n",
        "        p1 = p1.squeeze()\n",
        "        # [bs, ctx_len]\n",
        "        \n",
        "        #p1 = F.softmax(p1, dim=-1)\n",
        "        \n",
        "        # END PREDICTION\n",
        "        \n",
        "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
        "        # [bs, ctx_len, 1] => [bs, ctx_len]\n",
        "        \n",
        "        #p2 = F.softmax(p2, dim=-1)\n",
        "        \n",
        "        \n",
        "        return p1, p2\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fa1RTj5AXFkM"
      },
      "source": [
        "## Training\n",
        "\n",
        ">  *We use 100 1D ﬁlters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. The model has about 2.6 million parameters. We use the AdaDelta(Zeiler,2012) optimizer, with a mini batch size of 60 and an initial learning rate of 0.5, for 12 epochs. A dropout rate of 0.2 isused for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.*\n",
        "\n",
        "__Note__- Although the mini-batch size mentioned here is 60, you might need to change this depending on your GPU. The authors have trained this model on Titan X. I have used GTX 1080 Ti for training this model which has a RAM of 11.2 GB. I had to reduce my mini-batch size to 16 or 12 to make it work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3BNSWRXBXFkM"
      },
      "outputs": [],
      "source": [
        "CHAR_VOCAB_DIM = len(char2idx)\n",
        "EMB_DIM = 100\n",
        "CHAR_EMB_DIM = 8\n",
        "NUM_OUTPUT_CHANNELS = 100\n",
        "KERNEL_SIZE = (8,CHARACTER_CHANNEL_WIDTH)\n",
        "device = torch.device('cuda')\n",
        "\n",
        "model = BiDAF(CHAR_VOCAB_DIM, \n",
        "              EMB_DIM, \n",
        "              CHAR_EMB_DIM, \n",
        "              NUM_OUTPUT_CHANNELS, \n",
        "              KERNEL_SIZE, \n",
        "              HIDDEN_SIZE, \n",
        "              device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "EX-krn5AXFkM"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "RWbLDPdwXFkM"
      },
      "outputs": [],
      "source": [
        "def plot_grad_flow(named_parameters):\n",
        "    '''Plots the gradients flowing through different layers in the net during training.\n",
        "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
        "    \n",
        "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
        "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
        "    ave_grads = []\n",
        "    max_grads= []\n",
        "    layers = []\n",
        "    for n, p in named_parameters:\n",
        "        if(p.requires_grad) and (\"bias\" not in n):\n",
        "            layers.append(n)\n",
        "            ave_grads.append(p.grad.abs().mean())\n",
        "            max_grads.append(p.grad.abs().max())\n",
        "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
        "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
        "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
        "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
        "    plt.xlim(left=0, right=len(ave_grads))\n",
        "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
        "    plt.xlabel(\"Layers\")\n",
        "    plt.ylabel(\"average gradient\")\n",
        "    plt.title(\"Gradient flow\")\n",
        "    plt.grid(True)\n",
        "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
        "                Line2D([0], [0], color=\"b\", lw=4),\n",
        "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "-e7eiLTKXFkM"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "KjzYPIoFXFkN"
      },
      "outputs": [],
      "source": [
        "if OPTIMIZER==\"AdaDelta\":\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=LEARNING_RATE)\n",
        "elif OPTIMIZER==\"Adam\":\n",
        "    optimizer=optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "elif OPTIMIZER==\"SGD\":\n",
        "    optimizer=optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4) # momentum added\n",
        "else:\n",
        "    raise Exception(\"Unrecognized optimizer chosen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "1m0VdUxvXFkN"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "def train(model, train_dataset):\n",
        "    print(\"Starting training ........\")\n",
        "   \n",
        "    train_loss = 0.\n",
        "    batch_count = 0\n",
        "    model.train()\n",
        "    for batch in train_dataset:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        optimizer.zero_grad()\n",
        "    \n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch: {batch_count}\")\n",
        "        batch_count += 1\n",
        "        \n",
        "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
        "\n",
        "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
        "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
        "\n",
        "\n",
        "        preds = model(context, question, char_ctx, char_ques)\n",
        "\n",
        "        start_pred, end_pred = preds\n",
        "\n",
        "        s_idx, e_idx = label[:,0], label[:,1]\n",
        "\n",
        "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        #plot_grad_flow(model.named_parameters())\n",
        "        \"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if(param.requires_grad) and (\"bias\" not in name):\n",
        "                writer.add_histogram(name+'_grad',param.grad.abs().mean())\n",
        "        \"\"\"\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        del context, question, char_ctx, char_ques, label, ctx_text, ans, ids, preds, start_pred, end_pred,s_idx, e_idx\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss/len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "zEJCdFpJXFkN"
      },
      "outputs": [],
      "source": [
        "def valid(model, valid_dataset):\n",
        "    \n",
        "    print(\"Starting validation .........\")\n",
        "   \n",
        "    valid_loss = 0.\n",
        "\n",
        "    batch_count = 0\n",
        "    \n",
        "    f1, em = 0., 0.\n",
        "    \n",
        "    model.eval()\n",
        "        \n",
        "   \n",
        "    predictions = {}\n",
        "    \n",
        "    for batch in valid_dataset:\n",
        "\n",
        "        if batch_count % 500 == 0:\n",
        "            print(f\"Starting batch {batch_count}\")\n",
        "        batch_count += 1\n",
        "\n",
        "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\n",
        "\n",
        "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
        "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
        "        \n",
        "       \n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            s_idx, e_idx = label[:,0], label[:,1]\n",
        "\n",
        "            preds = model(context, question, char_ctx, char_ques)\n",
        "\n",
        "            p1, p2 = preds\n",
        "\n",
        "            \n",
        "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "            \n",
        "           \n",
        "            for i in range(batch_size):\n",
        "                id = ids[i]\n",
        "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
        "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
        "                predictions[id] = pred\n",
        "            \n",
        "\n",
        "    \n",
        "    em, f1 = evaluate(predictions)\n",
        "    return valid_loss/len(valid_dataset), em, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "NgNHepmzXFkN"
      },
      "outputs": [],
      "source": [
        "def evaluate(predictions):\n",
        "    '''\n",
        "    Gets a dictionary of predictions with question_id as key\n",
        "    and prediction as value. The validation dataset has multiple \n",
        "    answers for a single question. Hence we compare our prediction\n",
        "    with all the answers and choose the one that gives us\n",
        "    the maximum metric (em or f1). \n",
        "    This method first parses the JSON file, gets all the answers\n",
        "    for a given id and then passes the list of answers and the \n",
        "    predictions to calculate em, f1.\n",
        "    \n",
        "    \n",
        "    :param dict predictions\n",
        "    Returns\n",
        "    : exact_match: 1 if the prediction and ground truth \n",
        "      match exactly, 0 otherwise.\n",
        "    : f1_score: \n",
        "    '''\n",
        "    with open('./data/squad_dev.json','r',encoding='utf-8') as f:\n",
        "        dataset = json.load(f)\n",
        "        \n",
        "    dataset = dataset['data']\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    continue\n",
        "                \n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                \n",
        "                prediction = predictions[qa['id']]\n",
        "                \n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                \n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "                \n",
        "    \n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "    \n",
        "    return exact_match, f1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "D4HzBoeOXFkO"
      },
      "outputs": [],
      "source": [
        "def normalize_answer(s):\n",
        "    '''\n",
        "    Performs a series of cleaning steps on the ground truth and \n",
        "    predicted answer.\n",
        "    '''\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    '''\n",
        "    Returns maximum value of metrics for predicition by model against\n",
        "    multiple ground truths.\n",
        "    \n",
        "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
        "    :param str prediction: predicted answer span by the model\n",
        "    :param list ground_truths: list of ground truths against which\n",
        "                               metrics are calculated. Maximum values of \n",
        "                               metrics are chosen.\n",
        "                            \n",
        "    \n",
        "    '''\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "        \n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    '''\n",
        "    Returns f1 score of two strings.\n",
        "    '''\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    '''\n",
        "    Returns exact_match_score of two strings.\n",
        "    '''\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    '''\n",
        "    Helper function to record epoch time.\n",
        "    '''\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "IZUy6j7G_Pz9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e501817-6f8b-45a4-8257-03ac2d7ef5b5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "cEGSVxESXFkO",
        "outputId": "8ad76c2e-f840-4e26-e253-3ce3b37563f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Starting training ........\n",
            "Starting batch: 0\n",
            "Starting batch: 500\n",
            "Starting batch: 1000\n",
            "Starting batch: 1500\n",
            "Starting batch: 2000\n",
            "Starting batch: 2500\n",
            "Starting batch: 3000\n",
            "Starting batch: 3500\n",
            "Starting batch: 4000\n",
            "Starting batch: 4500\n",
            "Starting batch: 5000\n",
            "Starting validation .........\n",
            "Starting batch 0\n",
            "Starting batch 500\n",
            "Starting batch 1000\n",
            "Starting batch 1500\n",
            "Starting batch 2000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-bf8fdce54dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbest_valid_loss\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0mvalid_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# save the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         torch.save({\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m             \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mcontainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_zipfile_writer_buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_zipfile_writer_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Parent directory content/drive/MyDrive does not exist."
          ]
        }
      ],
      "source": [
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "ems = []\n",
        "f1s = []\n",
        "epochs = 5\n",
        "best_valid_loss=99999\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_dataset)\n",
        "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
        "    \n",
        "    \n",
        "    if best_valid_loss>valid_loss: # save the best model\n",
        "        torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': valid_loss,\n",
        "                'em':em,\n",
        "                'f1':f1,\n",
        "                }, f'drive/MyDrive/{model_name}.pth')\n",
        "        best_valid_loss=valid_loss\n",
        "\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    ems.append(em)\n",
        "    f1s.append(f1)\n",
        "\n",
        "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
        "    print(f\"Epoch valid loss: {valid_loss}\")\n",
        "    print(f\"Epoch EM: {em}\")\n",
        "    print(f\"Epoch F1: {f1}\")\n",
        "    print(\"====================================================================================\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if 0: # this prints the first epoch\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "    ems.append(em)\n",
        "    f1s.append(f1)\n",
        "\n",
        "    print(f\"Epoch train loss : {train_loss}\")\n",
        "    print(f\"Epoch valid loss: {valid_loss}\")\n",
        "    print(f\"Epoch EM: {em}\")\n",
        "    print(f\"Epoch F1: {f1}\")\n",
        "    print(\"====================================================================================\")\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQk0EgWclFZb",
        "outputId": "21e64c2a-ec59-43c1-c7c0-fa97dcc56059"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch train loss : 7.392903731774914\n",
            "Epoch valid loss: 6.675226976062709\n",
            "Epoch EM: 13.907284768211921\n",
            "Epoch F1: 22.64472337854702\n",
            "====================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8atlRxvXFkO"
      },
      "outputs": [],
      "source": [
        "train_loss\n",
        "print(valid_loss)\n",
        "print(em)\n",
        "print(f1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AcH8zBPF_ONO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVgPQ3w7XFkO"
      },
      "source": [
        "## References\n",
        "* Papers read/referred:\n",
        "    1. BiDAF: https://arxiv.org/abs/1611.01603\n",
        "    2. Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882\n",
        "    3. Highway Networks: https://arxiv.org/abs/1505.00387\n",
        "* Other helpful links:\n",
        "    1. https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here.\n",
        "    2. https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b. A great series of blogs to understand BiDAF.\n",
        "    Some of the following repos might be out of date.\n",
        "    3. https://github.com/allenai/bi-att-flow\n",
        "    4. https://github.com/galsang\n",
        "    5. https://github.com/jojonki/BiDAF/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}