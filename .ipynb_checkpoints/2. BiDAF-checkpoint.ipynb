{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDAF\n",
    "\n",
    "This notebook implements one of the most important papers in NLP literature: [BiDAF](https://arxiv.org/abs/1611.01603) or Bidirectional Attention Flow for Machine Comprehension. The key issue that this paper tries to address is that of *early summarization* in all the earlier approches that use attention mechanisms. The attention mechanisms until then were used to obtain a fixed-size summarization of given values and query. This, according to the authors leads to early summarization and loss of information. Moreover, previously, attention was only calculated in only one direction. To improve upon these issues, the authors propose a *hierarchical, multi-stage network*.   \n",
    "> *Our attention layer is not used to summarize the context paragraph into a ﬁxed-size vector. Instead, the attention is computed for every time step, and the attended vector at each time step, along with the representations from previous layers, is allowed to ﬂow through to the subsequent modeling layer. *\n",
    "\n",
    "Let's get into the intricacies of the model.\n",
    "The flow of this notebook will be similar to the previous one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.0-cp39-cp39-win_amd64.whl (8.3 MB)\n",
      "     ---------------------------------------- 8.3/8.3 MB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "Successfully installed scikit-learn-1.2.0 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached spacy-3.4.4-cp39-cp39-win_amd64.whl (11.9 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.5-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.8/96.8 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.5-cp39-cp39-win_amd64.whl (481 kB)\n",
      "     -------------------------------------- 481.4/481.4 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.6/181.6 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "     ---------------------------------------- 48.9/48.9 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.2-cp39-cp39-win_amd64.whl (2.1 MB)\n",
      "     ---------------------------------------- 2.1/2.1 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.10\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Collecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.3-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\gpans\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: wasabi, cymem, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.9 catalogue-2.0.8 confection-0.0.3 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.2 spacy-3.4.4 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 typer-0.7.0 wasabi-0.10.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\gpans\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, time\n",
    "import re, os, string, typing, gc, json\n",
    "import torch.nn.functional as F\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "nlp = spacy.blank('en')\n",
    "from preprocess import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of data:  442\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  University_of_Notre_Dame\n",
      "Length of data:  48\n",
      "Data Keys:  dict_keys(['title', 'paragraphs'])\n",
      "Title:  Super_Bowl_50\n",
      "--------------------------\n",
      "Train list len:  87599\n",
      "Valid list len:  34726\n"
     ]
    }
   ],
   "source": [
    "# load dataset json files\n",
    "\n",
    "train_data = load_json('./data/squad_train.json')\n",
    "valid_data = load_json('./data/squad_dev.json')\n",
    "\n",
    "# parse the json structure to return the data as a list of dictionaries\n",
    "\n",
    "train_list = parse_data(train_data)\n",
    "valid_list = parse_data(valid_data)\n",
    "print('--------------------------')\n",
    "\n",
    "print('Train list len: ',len(train_list))\n",
    "print('Valid list len: ',len(valid_list))\n",
    "\n",
    "# converting the lists into dataframes\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "valid_df = pd.DataFrame(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5733be284776f41900661182</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>[515, 541]</td>\n",
       "      <td>Saint Bernadette Soubirous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5733be284776f4190066117f</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>[188, 213]</td>\n",
       "      <td>a copper statue of Christ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5733be284776f41900661180</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>[279, 296]</td>\n",
       "      <td>the Main Building</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5733be284776f41900661181</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>[381, 420]</td>\n",
       "      <td>a Marian place of prayer and reflection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5733be284776f4190066117e</td>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>[92, 126]</td>\n",
       "      <td>a golden statue of the Virgin Mary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  5733be284776f41900661182   \n",
       "1  5733be284776f4190066117f   \n",
       "2  5733be284776f41900661180   \n",
       "3  5733be284776f41900661181   \n",
       "4  5733be284776f4190066117e   \n",
       "\n",
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question       label  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...  [515, 541]   \n",
       "1  What is in front of the Notre Dame Main Building?  [188, 213]   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...  [279, 296]   \n",
       "3                  What is the Grotto at Notre Dame?  [381, 420]   \n",
       "4  What sits on top of the Main Building at Notre...   [92, 126]   \n",
       "\n",
       "                                    answer  \n",
       "0               Saint Bernadette Soubirous  \n",
       "1                a copper statue of Christ  \n",
       "2                        the Main Building  \n",
       "3  a Marian place of prayer and reflection  \n",
       "4       a golden statue of the Virgin Mary  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    \n",
    "    def to_lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    df.context = df.context.apply(to_lower)\n",
    "    df.question = df.question.apply(to_lower)\n",
    "    df.answer = df.answer.apply(to_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_df(train_df)\n",
    "preprocess_df(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 453 ms\n",
      "Wall time: 449 ms\n",
      "Number of sentences in dataset:  118822\n"
     ]
    }
   ],
   "source": [
    "# gather text to build vocabularies\n",
    "\n",
    "%time vocab_text = gather_text_for_vocab([train_df, valid_df])\n",
    "print(\"Number of sentences in dataset: \", len(vocab_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-vocab: 96774\n",
      "vocab-length: 96776\n",
      "word2idx-length: 96776\n",
      "CPU times: total: 7.56 s\n",
      "Wall time: 7.6 s\n",
      "----------------------------------\n",
      "raw-char-vocab: 1316\n",
      "char-vocab-intersect: 202\n",
      "char2idx-length: 204\n",
      "CPU times: total: 2.33 s\n",
      "Wall time: 2.32 s\n"
     ]
    }
   ],
   "source": [
    "# build word and character-level vocabularies\n",
    "\n",
    "%time word2idx, idx2word, word_vocab = build_word_vocab(vocab_text)\n",
    "print(\"----------------------------------\")\n",
    "%time char2idx, char_vocab = build_char_vocab(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.2 s\n",
      "Wall time: 15.3 s\n",
      "CPU times: total: 6.55 s\n",
      "Wall time: 6.56 s\n",
      "CPU times: total: 2.34 s\n",
      "Wall time: 2.35 s\n",
      "CPU times: total: 891 ms\n",
      "Wall time: 887 ms\n"
     ]
    }
   ],
   "source": [
    "# numericalize context and questions for training and validation set\n",
    "\n",
    "%time train_df['context_ids'] = train_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time valid_df['context_ids'] = valid_df.context.apply(context_to_ids, word2idx=word2idx)\n",
    "%time train_df['question_ids'] = train_df.question.apply(question_to_ids, word2idx=word2idx)\n",
    "%time valid_df['question_ids'] = valid_df.question.apply(question_to_ids, word2idx=word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of error indices: 921\n",
      "Number of error indices: 349\n"
     ]
    }
   ],
   "source": [
    "# get indices with tokenization errors and drop those indices \n",
    "\n",
    "train_err = get_error_indices(train_df, idx2word)\n",
    "valid_err = get_error_indices(valid_df, idx2word)\n",
    "\n",
    "train_df.drop(train_err, inplace=True)\n",
    "valid_df.drop(valid_err, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get start and end positions of answers from the context\n",
    "# this is basically the label for training QA models\n",
    "\n",
    "train_label_idx = train_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "valid_label_idx = valid_df.apply(index_answer, axis=1, idx2word=idx2word)\n",
    "\n",
    "train_df['label_idx'] = train_label_idx\n",
    "valid_df['label_idx'] = valid_label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump to pickle files\n",
    "\n",
    "train_df.to_pickle('bidaftrain.pkl')\n",
    "valid_df.to_pickle('bidafvalid.pkl')\n",
    "\n",
    "with open('bidafw2id.pickle','wb') as handle:\n",
    "    pickle.dump(word2idx, handle)\n",
    "\n",
    "with open('bidafc2id.pickle','wb') as handle:\n",
    "    pickle.dump(char2idx, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle files\n",
    "\n",
    "\n",
    "train_df = pd.read_pickle('bidaftrain.pkl')\n",
    "valid_df = pd.read_pickle('bidafvalid.pkl')\n",
    "\n",
    "with open('bidafw2id.pickle','rb') as handle:\n",
    "    word2idx = pickle.load(handle)\n",
    "with open('bidafc2id.pickle','rb') as handle:\n",
    "    char2idx = pickle.load(handle)\n",
    "\n",
    "idx2word = {v:k for k,v in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader/Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset:\n",
    "    '''\n",
    "    - Creates batches dynamically by padding to the length of largest example\n",
    "      in a given batch.\n",
    "    - Calulates character vectors for contexts and question.\n",
    "    - Returns tensors for training.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, data, batch_size):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        data = [data[i:i+self.batch_size] for i in range(0, len(data), self.batch_size)]\n",
    "        self.data = data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def make_char_vector(self, max_sent_len, max_word_len, sentence):\n",
    "        \n",
    "        char_vec = torch.ones(max_sent_len, max_word_len).type(torch.LongTensor)\n",
    "        \n",
    "        for i, word in enumerate(nlp(sentence, disable=['parser','tagger','ner',\"lemmatizer\"])):\n",
    "            for j, ch in enumerate(word.text):\n",
    "                char_vec[i][j] = char2idx.get(ch, 0)\n",
    "        \n",
    "        return char_vec    \n",
    "    \n",
    "    def get_span(self, text):\n",
    "        \n",
    "        text = nlp(text, disable=['parser','tagger','ner'])\n",
    "        span = [(w.idx, w.idx+len(w.text)) for w in text]\n",
    "\n",
    "        return span\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''\n",
    "        Creates batches of data and yields them.\n",
    "        \n",
    "        Each yield comprises of:\n",
    "        :padded_context: padded tensor of contexts for each batch \n",
    "        :padded_question: padded tensor of questions for each batch \n",
    "        :char_ctx & ques_ctx: character-level ids for context and question\n",
    "        :label: start and end index wrt context_ids\n",
    "        :context_text,answer_text: used while validation to calculate metrics\n",
    "        :ids: question_ids for evaluation\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        for batch in self.data:\n",
    "            \n",
    "            spans = []\n",
    "            ctx_text = []\n",
    "            answer_text = []\n",
    "            \n",
    "            for ctx in batch.context:\n",
    "                ctx_text.append(ctx)\n",
    "                spans.append(self.get_span(ctx))\n",
    "            \n",
    "            for ans in batch.answer:\n",
    "                answer_text.append(ans)\n",
    "                \n",
    "            \n",
    "            max_context_len = max([len(ctx) for ctx in batch.context_ids])\n",
    "            padded_context = torch.LongTensor(len(batch), max_context_len).fill_(1)\n",
    "            \n",
    "            for i, ctx in enumerate(batch.context_ids):\n",
    "                padded_context[i, :len(ctx)] = torch.LongTensor(ctx)\n",
    "                \n",
    "            max_word_ctx = 0\n",
    "            for context in batch.context:\n",
    "                for word in nlp(context, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ctx:\n",
    "                        max_word_ctx = len(word.text)\n",
    "            \n",
    "            char_ctx = torch.ones(len(batch), max_context_len, max_word_ctx).type(torch.LongTensor)\n",
    "            for i, context in enumerate(batch.context):\n",
    "                char_ctx[i] = self.make_char_vector(max_context_len, max_word_ctx, context)\n",
    "            \n",
    "            max_question_len = max([len(ques) for ques in batch.question_ids])\n",
    "            padded_question = torch.LongTensor(len(batch), max_question_len).fill_(1)\n",
    "            \n",
    "            for i, ques in enumerate(batch.question_ids):\n",
    "                padded_question[i, :len(ques)] = torch.LongTensor(ques)\n",
    "                \n",
    "            max_word_ques = 0\n",
    "            for question in batch.question:\n",
    "                for word in nlp(question, disable=['parser','tagger','ner']):\n",
    "                    if len(word.text) > max_word_ques:\n",
    "                        max_word_ques = len(word.text)\n",
    "            \n",
    "            char_ques = torch.ones(len(batch), max_question_len, max_word_ques).type(torch.LongTensor)\n",
    "            for i, question in enumerate(batch.question):\n",
    "                char_ques[i] = self.make_char_vector(max_question_len, max_word_ques, question)\n",
    "            \n",
    "            ids = list(batch.id)  \n",
    "            label = torch.LongTensor(list(batch.label_idx))\n",
    "            \n",
    "            yield (padded_context, padded_question, char_ctx, char_ques, label, ctx_text, answer_text, ids)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_df, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = SquadDataset(valid_df, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiDAF Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding\n",
    "\n",
    "> *Word embedding layer also maps each word to a high-dimensional vector space. We use pre-trained word vectors, GloVe to obtain the ﬁxed word embedding of each word. *\n",
    "\n",
    "This model uses 100-dimensional pre-trained word vectors. The `weights_matrix` obtained below is initialized as an `nn.Embedding`'s weight. This is done in the last module in a function which as follows:\n",
    "\n",
    "```\n",
    "weights_matrix = np.load('bidafglove.npy')\n",
    "num_embeddings, embedding_dim = weights_matrix.shape\n",
    "embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' 不是內部或外部命令、可執行的程式或批次檔。\n"
     ]
    }
   ],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dict():\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    encoding = 'utf-8'\n",
    "    glove_dict = {}\n",
    "    archive = zipfile.ZipFile('glove.6B.zip', 'r')\n",
    "    with archive.open(\"glove.6B.100d.txt\", \"r\") as f:\n",
    "        for line in io.TextIOWrapper(f, encoding):\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            glove_dict[word] = vector\n",
    "            \n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_dict_local_machine(): # assumes zip downloaded\n",
    "    '''\n",
    "    Parses the glove word vectors text file and returns a dictionary with the words as\n",
    "    keys and their respective pretrained word vectors as values.\n",
    "\n",
    "    '''\n",
    "    glove_dict = {}\n",
    "    with open(\"./glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            glove_dict[word] = vector\n",
    "            \n",
    "    f.close()\n",
    "    \n",
    "    return glove_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = get_glove_dict_local_machine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_weights_matrix(glove_dict):\n",
    "    '''\n",
    "    Creates a weight matrix of the words that are common in the GloVe vocab and\n",
    "    the dataset's vocab. Initializes OOV words with a zero vector.\n",
    "    '''\n",
    "    weights_matrix = np.zeros((len(word_vocab), 100))\n",
    "    words_found = 0\n",
    "    for i, word in enumerate(word_vocab):\n",
    "        try:\n",
    "            weights_matrix[i] = glove_dict[word]\n",
    "            words_found += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return weights_matrix, words_found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words found in the GloVe vocab:  72687\n"
     ]
    }
   ],
   "source": [
    "weights_matrix, words_found = create_weights_matrix(glove_dict)\n",
    "print(\"Words found in the GloVe vocab: \" ,words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump the weights to load in future\n",
    "\n",
    "np.save('bidafglove_tv.npy', weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Embedding\n",
    "\n",
    "A character embedding is calculated for each context and query word. This is done by using convolutions.   \n",
    ">  *It maps each word to a vector space using character-level CNNs.*\n",
    "\n",
    "Using CNNs in NLP was first proposed by Yoon Kim in 2014 in his paper \"Convolutional Neural Networks for Sentence Classification\". This paper tries to use CNNs in NLP as they are used in vision. Most of the state-of-the-art results in CV at that time were achieved by transfer learning from larger models pretrained on ImageNet. In this paper, they train a simple CNN with one layer of convolution on top of pretrained word vectors and hypothesized that these pretrained word vectors could work as a universal feature extractors for various classification tasks. This is analogous to the earlier layers of vision models like VGG and Inception working as generic feature extractors.\n",
    "\n",
    "The intuition is simple over here. Just as convolutional filters learn various features in an image by operating on its pixels, here they'll do so by operating on characters of words. Let's get into the working of this layer.  \n",
    "\n",
    "We first pass each word through an embedding layer to get a fixed size vector. Let the embedding dimension be $d$.\n",
    "Let $C$ represent a matrix representation of word of length $l$. Therefore $C$ is a matrix with dimensions $d$ x $l$.\n",
    "<img src=\"images/charemb1.PNG\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "Let $H$ represent a convolutional filter with dimensions $d$ x $w$, where $d$ is the embedding dimension and $w$ is the width or the window size of the filter.   \n",
    "The weights of this filter are randomly initialized and learnt parallelly via backpropogation. We convolve this filter $H$ over our word representation $C$ as shown below. \n",
    "<img src=\"images/charemb2.PNG\" width=\"500\" height=\"400\"/>  \n",
    "\n",
    "The convolution operation is simply the inner product of the filter $H$ and matrix $C$. The convolution operations can be visualized as follows:\n",
    "<img src=\"images/charemb3.PNG\" width=\"900\" height=\"700\"/>    \n",
    "The result of the above operation is a feature vector. A single filter is usually associated with a unique feature that it captures from the image/matrix. To get the most representative value related to the feature, we perform max pooling over the dimension of this vector.\n",
    "<img src=\"images/charemb4.PNG\" width=\"500\" height=\"300\"/>     \n",
    "\n",
    "The above process was described for a single filter. This same process is repeated with $N$ number of filters. Each of these filters captures a different property of word. In an image, for example, if one filter captures the edges, another filter will capture the texture and another one the shapes in the image and so on. $N$ is also the size of the desired character embedding. In this paper authors have trained the model with $N$ = 100.  \n",
    "\n",
    "The implementation of this layer is fairly straightforward.  The input to this layer is of dimension `[batch_size, seq_len, word_len]` where `seq_len` and `word_len` are the lengths of largest sequence and word respectively within a given batch . We first embed the character tokens into a fixed size vector using an embedding layer. This gives a vector of dimension `[batch_size, seq_len, word_len, emb_dim]`.   \n",
    "We then convert this tensor into a format that closely resembles an image, of type [ $N$, $C_{in}$, $H_{in}$, $W_{in}$]. The number of input channels, $C_{in}$ would be 1 and the output channels would be the desired embedding size which is 100. This is then passed through the convolution layer which gives an output of shape [ $N$, $C_{out}$, $H_{out}$, $W_{out}$]. Here,\n",
    "\n",
    "<img src=\"images/conv.PNG\" width=\"600\" height=\"500\"/>  \n",
    "\n",
    "If `padding` = [0,0], `kernel_size` (or filter_size) = [$H_{in}$, $w$], `dilation` = [1,1], `stride` = [1,1] (as visible in images above), then,   \n",
    "$H_{out}$ = 1, and $W_{out}$ = $W_{in}$ - $w$ - 1.\n",
    "\n",
    "Since $H_{out}$ = 1, we squeeze that dimension and perform max pooling with a kernel_size = $L_{in}$. The value of $L_{in}$ =  $W_{in}$ - $w$ - 1.\n",
    "\n",
    "<img src=\"images/maxpool.PNG\" width=\"600\" height=\"500\"/>     \n",
    "If the kernel size = $L_{in}$, we get $L_{out}$ = 1 if other values are default. This dimension is again squeezed to finally give us a tensor of dimension   `[batch_size, seq_len, output_channels (or 100)]`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use this as my reference. DO NOT RUN!!!\n",
    "\n",
    "DROPOUT_RATE=0.2 # ANCHOR default: 0.2\n",
    "BATCH_SIZE=None # Use None to get default x.shape[0]\n",
    "CHARACTER_CHANNEL_WIDTH=5 # default 5\n",
    "HIDDEN_SIZE=100 # default 100\n",
    "LEARNING_RATE = 0.01 # default: 0.01 in SGD, 1 in AdaDelta, 1e-3 in Adam\n",
    "OPTIMIZER=\"AdaDelta\"\n",
    "model_name=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will test parameters: learning rate and optimizer individually as they are relatively isolated from other variables but very close to each other. The state dictionary, loss, EM, F1 can be found under [model_name].pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_configuration=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_config_list=[]\n",
    "# configuration 0\n",
    "optimizer_config_list.append((\"AdaDelta\",1,\"AdaDelta_default\"))\n",
    "# configuration 1\n",
    "optimizer_config_list.append((\"AdaDelta\",2,\"AdaDelta_doubled\"))\n",
    "# configuration 2\n",
    "optimizer_config_list.append((\"AdaDelta\",.5,\"AdaDelta_halved\"))\n",
    "\n",
    "# configuration 3\n",
    "optimizer_config_list.append((\"Adam\",.001,\"Adam_default\"))\n",
    "# configuration 4\n",
    "optimizer_config_list.append((\"Adam\",.002,\"Adam_doubled\"))\n",
    "# configuration 5\n",
    "optimizer_config_list.append((\"Adam\",.0005,\"Adam_halved\"))\n",
    "\n",
    "# configuration 6\n",
    "optimizer_config_list.append((\"SGD\",.01,\"SGD_default\"))\n",
    "# configuration 7\n",
    "optimizer_config_list.append((\"SGD\",.02,\"SGD_doubled\"))\n",
    "# configuration 8\n",
    "optimizer_config_list.append((\"SGD\",.005,\"SGD_halved\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp \n",
    "DROPOUT_RATE=0.2 # ANCHOR default: 0.2\n",
    "BATCH_SIZE=None # Use None to get default x.shape[0]\n",
    "CHARACTER_CHANNEL_WIDTH=5 # default 5\n",
    "HIDDEN_SIZE=100 # default 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER, LEARNING_RATE, model_name = optimizer_config_list[optim_configuration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have determined that configuration X that has optimizer X with learning rate of X works best for this model with a F1 score of X and EM score of X after 5 epochs. We will then move onto batch size, which is also very much separated from other parameters.\n",
    "\n",
    "Note: model_name will be overwritten here. Please assume the optimizer and learning rate picked is the configuration X with the best overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, char_emb_dim, num_output_channels, kernel_size, batch_size=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.char_emb_dim = char_emb_dim\n",
    "        \n",
    "        self.char_embedding = nn.Embedding(char_vocab_dim, char_emb_dim, padding_idx=1)\n",
    "        \n",
    "        self.char_convolution = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=kernel_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        \n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, word_len]\n",
    "        # returns : [batch_size, seq_len, num_output_channels]\n",
    "        # the output can be thought of as another feature embedding of dim 100.\n",
    "        \n",
    "        batch_size = x.shape[0]//2 # try this to see if this solves out of memory error\n",
    "        print(f\"batch size: {batch_size}\")\n",
    "        \n",
    "        if (self.batch_size):\n",
    "            batch_size=self.batch_size\n",
    "        \n",
    "        x = self.dropout(self.char_embedding(x))\n",
    "        # x = [bs, seq_len, word_len, char_emb_dim]\n",
    "        \n",
    "        # following three operations manipulate x in such a way that\n",
    "        # it closely resembles an image. this format is important before \n",
    "        # we perform convolution on the character embeddings.\n",
    "        \n",
    "        x = x.permute(0,1,3,2)\n",
    "        # x = [bs, seq_len, char_emb_dim, word_len]\n",
    "        \n",
    "        x = x.view(-1, self.char_emb_dim, x.shape[3])\n",
    "        # x = [bs*seq_len, char_emb_dim, word_len]\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "        # x = [bs*seq_len, 1, char_emb_dim, word_len]\n",
    "        \n",
    "        # x is now in a format that can be accepted by a conv layer. \n",
    "        # think of the tensor above in terms of an image of dimension\n",
    "        # (N, C_in, H_in, W_in).\n",
    "        \n",
    "        x = self.relu(self.char_convolution(x))\n",
    "        # x = [bs*seq_len, out_channels, H_out, W_out]\n",
    "        \n",
    "        x = x.squeeze()\n",
    "        # x = [bs*seq_len, out_channels, W_out]\n",
    "                \n",
    "        x = F.max_pool1d(x, x.shape[2]).squeeze()\n",
    "        # x = [bs*seq_len, out_channels, 1] => [bs*seq_len, out_channels]\n",
    "        \n",
    "        x = x.view(batch_size, -1, x.shape[-1])\n",
    "        # x = [bs, seq_len, out_channels]\n",
    "        # x = [bs, seq_len, features] = [bs, seq_len, 100]\n",
    "        \n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Networks\n",
    "\n",
    "Highway networks were originally introduced to ease the training of deep neural networks. While researchers had cracked the code for optimizing shallow neural networks, training *deep* networks was still a challenging task owing to problems such as vanishing gradients etc. Quoting the paper,\n",
    "\n",
    ">  *We present a novel architecture that enables the optimization of networks with virtually arbitrary depth. This is accomplished through the use of a learned gating mechanism for regulating information ﬂow which is inspired by Long Short Term Memory recurrent neural networks. Due to this gating mechanism, a neural network can have paths along which information can ﬂow across several layers without attenuation. We call such paths information highways, and such networks highway networks.* \n",
    "\n",
    "This paper takes the key idea of learned gating mechanism from LSTMs which process information internally through a sequence of learned gates. The purpose of this layer is to *learn* to pass relevant information from the input. A highway network is a series of feed-forward or linear layers with a gating mechanism. The gating is implemented by using a sigmoid function which decides what amount of information should be transformed and what should be passed as it is.   \n",
    "\n",
    "A plain feed-forward layer is associated with a linear transform $H$ parameterized by ($W_{H}, b_{H}$), such that for input $x$, the output $y$ is  \n",
    "\n",
    "$$ y = g(W_{H}.x + b_{H})$$\n",
    "where $g$ is a non-linear activation.  \n",
    "For highway networks, two additional linear transforms are defined viz. $T$ ($W_{T},b_{T}$) and $C$ ($W_{C}$,$b_{C}$).\n",
    "Then,    \n",
    "  \n",
    "$$ y = T(x) . H(x) + x . C(x) $$ \n",
    "> *We refer to T as the transform gate and C as the carry gate, since they express how much of the output is produced by\n",
    "transforming the input and carrying it, respectively. For simplicity, in this paper we set C = 1 − T. *\n",
    "\n",
    "$$ y = T(x) . H(x) + x . (1 - T(x)) $$  \n",
    "  \n",
    "$$ y = T(x) . g(W_{H}.x + b_{H}) + x . (1 - T(x)) $$  \n",
    "where $T(x)$ = $\\sigma$ ($W_{T}$ . $x$ + $b_{T}$) and $g$ is relu activation.  \n",
    "\n",
    "The input to this layer is the concatenation of word and character embeddings of each word. To implement this we use `nn.ModuleList` to add multiple linear layers. This is done for the gate layer as well as for a normal linear transform. In code the `flow_layer` is the same as linear transform $H$ discussed above and `gate_layer` is $T$. In the forward method we loop through each layer and compute the output according to the highway equation described above.   \n",
    "  \n",
    "The output of this layer for context is $X$ $\\epsilon$ $R^{\\ d \\ X \\ T}$ and for query is $Q$ $\\epsilon$ $R^{\\ d \\ X \\ J}$, where $d$ is hidden size of the LSTM, $T$ is the context length, $J$ is the query length.  \n",
    "\n",
    "The structure discussed so far is a recurring pattern in many NLP systems. Although this might be out of favor now with the advent of transformers and large pretrained language models, you will find this pattern in many NLP systems before transformers came into being. The idea behind this is that adding highway layers enables the network to make more efficient use of character embeddings. If a particular word is not found in the pretrained word vector vocabulary (OOV word), it will most likely be initialized with a zero vector. It then makes much more sense to look at the character embedding of that word rather than the word embedding. The soft gating mechanism in highway layers helps the model to achieve this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_layers=2):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.flow_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        self.gate_layer = nn.ModuleList([nn.Linear(input_dim, input_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            \n",
    "            flow_value = F.relu(self.flow_layer[i](x))\n",
    "            gate_value = torch.sigmoid(self.gate_layer[i](x))\n",
    "            \n",
    "            x = gate_value * flow_value + (1-gate_value) * x\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual Embedding\n",
    "\n",
    "This layer is the final embedding layer in the model.\n",
    "> *Bi-Directional Attention Flow (BIDAF) network, a hierarchical multi-stage architecture for modeling the representations ofthe context paragraph at different levels of granularity. BIDAF includes character-level, word-level, and contextual embeddings*\n",
    "\n",
    "The output of highway layers is passed to a bidirection LSTM to model the temporal features of the text. This is done for both, the context and the query.   \n",
    ">  *Utilizes contextual cues from surrounding words to reﬁne the embedding of the words*\n",
    "\n",
    "The output of this layer for context is called as $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and for query it is named as $U$ $\\epsilon$ $R^{\\ 2d \\  X \\  J}$. The $2d$ is because of the features from both forward and backward LSTMs. In code, we simply use the outputs of the LSTM layer and ignore the hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextualEmbeddingLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.highway_net = HighwayNetwork(input_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = [bs, seq_len, input_dim] = [bs, seq_len, emb_dim*2]\n",
    "        # the input is the concatenation of word and characeter embeddings\n",
    "        # for the sequence.\n",
    "        \n",
    "        highway_out = self.highway_net(x)\n",
    "        # highway_out = [bs, seq_len, input_dim]\n",
    "        \n",
    "        outputs, _ = self.lstm(highway_out)\n",
    "        # outputs = [bs, seq_len, emb_dim*2]\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Attention Flow Layer\n",
    "\n",
    "\n",
    "> *It is worth noting that the ﬁrst three layers of the model are computing features from the query and context at different levels of granularity, akin to the multi-stage feature computation of convolutional neural networks in the computer vision ﬁeld.*\n",
    "\n",
    "The output of the previous layer: contextual representation of context $H$ and query $U$ are passed on to this layer. Until now processing of the context and the query has been independent of each other. This layer, however, is responsible for fusing and linking the context and query representations.   \n",
    "\n",
    "This layer calculates attention in two directions: from context to query and from query to context. Attention vectors for these calculations are derived from a common matrix which is called as the similarity matrix and is denoted by $S$ $\\epsilon$ $R^{\\ T \\ X \\ J}$. $T$ is the length of the context (number of words/tokens) and $J$ is the length of the query for each training example. The similarity matrix is computed by,\n",
    "$$ S_{tj} = \\alpha\\ (H_{:t}, U_{:j}) $$  \n",
    "\n",
    "where $S_{tj}$ $\\epsilon$ $R$.  \n",
    "$S_{tj}$ is a single float value that determines the similarity between the $t$-th context word and $j$-th query word.\n",
    "$\\alpha$ is a trainable function that encodes the similarity between two input vectors $H_{:t}$ and $U_{:j}$.  \n",
    "$H_{:t}$ is the contextual representation of the $t$-th context word and $U_{:j}$ is the contextual representation of the $j$-th query word. \n",
    "<img src=\"images/simimat.PNG\" width=\"500\" height=\"500\"/>     \n",
    "\n",
    "\n",
    "\n",
    "The trainable function is defined as,  \n",
    "\n",
    "$$ \\alpha \\ (h, u) = w_{(S)}^{T}\\ [h\\ ;\\ u \\ ; \\ h \\circ u]  $$\n",
    "where $;$ denotes concatenation and $\\circ$ denotes an element wise product.    \n",
    "$w_{T}^{S}$ is a trainable weight matrix of dimension $6d$. This is because $H$ $\\epsilon$ $R^{\\ 2d \\ X \\ T}$ and $U$ $\\epsilon$ $R^{\\ 2d \\ X \\ J}$, and we are concatenating 3 such vectors.  \n",
    "\n",
    "In code, all the computations above are performed directly by operating on matrices/ tensors. We first `repeat` the contextual representations of context and query along appropriate dimensions to get two tensors of shape `[batch_size, ctx_len, query_len, emb_dim*2 (d*2)]`. $w_{(S)}^T$ is characterized by a linear layer called `similarity_weight`. We concat the tensors along the last dimension and pass it through the linear layer. \n",
    "\n",
    "### Context-to-Query Attention\n",
    "\n",
    "> *. Context-to-query (C2Q) attention signiﬁes which query words are most relevant to each context word.*\n",
    " \n",
    "Let $a_{t}$ represent the vector that encodes the attention paid on each query word by the $t$-th context word.   \n",
    "$$ \\sum_{j}a_{tj} = 1  $$  \n",
    "$$ a_{t} = softmax(S_{:t}) $$\n",
    "where $a_{t}$ $\\epsilon$ $R^{J}$   \n",
    "This can be visualized as,\n",
    "<img src=\"images/c2q.PNG\" width=\"700\" height=\"750\"/>     \n",
    "\n",
    "Subsequently, each attended query vector is calculated by,\n",
    "$$ \\overline U_{:t} = \\sum_{j} a_{tj} U_{:j} $$\n",
    "\n",
    "This vector indicates the most important word in the query with respect to context.  \n",
    "\n",
    "In code, again vectorizing operations into tensors, this simply involves taking a softmax of the similarity matrix across the last dimension, i.e across the columns and multiplying this with $U$. The shape of similarity matrix is `[batch_size, ctx_len, query_len]`. The shape of $U$ is `[batch_size, query_len, emb_dim*2]` . Hence performing matrix multiplication by using `torch.bmm` would give us a tensor of shape `[batch_size, ctx_len, emb_dim*2 ]`. Therefore, $\\overline U$ will be $2d$ X $T$\n",
    " matrix.\n",
    "\n",
    "\n",
    "### Query-to-Context Attention\n",
    "\n",
    ">  *Query-to-context (Q2C) attention signiﬁes which context words have the closest similarity to one of the query words and are hence critical for answering the query.*\n",
    "\n",
    "The attention vector here is calculated as,  \n",
    "$$ b = softmax \\ (max_{col} \\ (S)) \\ \\epsilon R^{T} $$  \n",
    "where $max_{col}$ performs the maximum function across the column. The attended context vector is then calculated as\n",
    "\n",
    "$$ \\overline h = \\sum_{t} b_{t} H_{:t}$$\n",
    "\n",
    "The above equations can be visualized as,\n",
    "\n",
    "<img src=\"images/q2c.PNG\" width=\"800\" height=\"900\"/>\n",
    "\n",
    "The above figure helps us in understanding that for each training example we'll get a single $T$ - dimensional vector. This vector will then be multiplied by the contextual representation $H$ to get a single $2d$ vector.  To get the matrix $\\overline H$, we need to tile/repeat this vector $T$ times to get a $2d$-by-$T$ representation.\n",
    "This is unlike the previous attention we calculated where we directly got a $2d$-by-$T$ matrix.\n",
    "\n",
    "The implementation for this part is straightforward and very similar to the explanation above. We first perform maximum across columns using `torch.max`. This gives a tensor of shape `[batch_size, ctx_len]`. This is then passed through a softmax to get weights that add up to 1. We then insert an extra dimension in this tensor and multiply it with $H$. This gives a tensor of shape `[batch_size, 1, emb_dim*2]`. This is exactly what we have discussed above. This tensor corresponds to $\\overline h$. We then `repeat` this $T$ times to get $\\overline H$ of shape `[batch_size, ctx_len, emb_dim*2]`\n",
    "\n",
    "\n",
    "### Combining attention vectors and contextual embeddings\n",
    "\n",
    "> *Finally, the contextual embeddings and the attention vectors are combined together to yield G, where each column vector can be considered as the query-aware representation of each context word.*\n",
    "\n",
    "$G$ is defined as,\n",
    "\n",
    "$$ G_{:t} = \\beta \\ (\\ H_{:t}\\ ;\\ \\overline U_{:t} \\ ;\\ \\overline H_{:t}  \\ )$$\n",
    "\n",
    "where $\\beta$ is a trainable function.  \n",
    "$$ \\beta (h, \\overline u, \\overline h) = [h\\ ;\\ \\overline u\\ ;\\ h \\circ \\overline u\\ ;\\ h \\circ \\overline h\\ ] $$\n",
    "\n",
    "This concatenation gives a $8d$ vector. Hence the trainable weight here should have an input dimension of $8d$. However here we don't involve a trainable weight because according to the authors,\n",
    "\n",
    "> *While the β function can be an arbitrary trainable neural network, such as multi-layer perceptron, a simple concatenation as following still shows good performance in our experiments.*\n",
    "\n",
    "In code, this simply involves a single line of concatenating tensors using `torch.cat` to yeild $G$. This tensor holds the query-aware representation of the context words.\n",
    "\n",
    "\n",
    "## Modeling Layer\n",
    "\n",
    "$G$ is then passed on to this layer. This layer is responsible for capturing temporal features interactions among the context words. This is done using a bidirectional LSTM. The difference between this layer and the contextual layer, both of which involve an LSTM layer is that here we have a *query-aware* representation of the context while in the contextual layer, encoding of the context and query was independent.  \n",
    "Using an LSTM layer with a hidden size of $d$ we get a $2d$-by-$T$ output called $M$.  \n",
    "\n",
    "> *Each column vector of M is expected to contain contextual information about the word with respect to the entire context paragraph and the query.*\n",
    "\n",
    "\n",
    "## Output Layer\n",
    "\n",
    "The start index $p^{1}$ of the answer span is calculated by,\n",
    "\n",
    "<img src=\"images/p1.PNG\" width=\"300\" height=\"300\"/>\n",
    "\n",
    "where $w_{(p^{1})}$ is a $10d$ trainable weight vector which corresponds to a linear layer in code called `output_start`.  \n",
    "To predict the end of the answer span, $M$ is once again passed to a bidirectional LSTM to get $M^{2}$ which again is a $2d$-by-$T$ matrix. The end is then computed as,  \n",
    "\n",
    "<img src=\"images/p2.PNG\" width=\"300\" height=\"300\"/>  \n",
    "\n",
    "\n",
    "$w_{(p^{2})}$ is again a $10d$ trainable weight vector and corresponds to the linear layer `output_end`. \n",
    "\n",
    "In code we don't explicitly calculate softmax since this is taken care of while calculating the losses during training. \n",
    "\n",
    "> *The output layer is application-speciﬁc. The modular nature of BIDAF allows us to easily swap out the output layer based on the task, with the rest of the architecture remaining exactly the same. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiDAF(nn.Module):\n",
    "    \n",
    "    def __init__(self, char_vocab_dim, emb_dim, char_emb_dim, num_output_channels, \n",
    "                 kernel_size, ctx_hidden_dim, device):\n",
    "        '''\n",
    "        char_vocab_dim = len(char2idx)\n",
    "        emb_dim = 100\n",
    "        char_emb_dim = 8\n",
    "        num_output_chanels = 100\n",
    "        kernel_size = (8,5)\n",
    "        ctx_hidden_dim = 100\n",
    "        '''\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.word_embedding = self.get_glove_embedding()\n",
    "        \n",
    "        self.character_embedding = CharacterEmbeddingLayer(char_vocab_dim, char_emb_dim, \n",
    "                                                      num_output_channels, kernel_size, BATCH_SIZE)\n",
    "        \n",
    "        self.contextual_embedding = ContextualEmbeddingLayer(emb_dim*2, ctx_hidden_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        self.similarity_weight = nn.Linear(emb_dim*6, 1, bias=False)\n",
    "        \n",
    "        self.modeling_lstm = nn.LSTM(emb_dim*8, emb_dim, bidirectional=True, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        self.output_start = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.output_end = nn.Linear(emb_dim*10, 1, bias=False)\n",
    "        \n",
    "        self.end_lstm = nn.LSTM(emb_dim*2, emb_dim, bidirectional=True, batch_first=True)\n",
    "        \n",
    "    \n",
    "    def get_glove_embedding(self):\n",
    "        \n",
    "        weights_matrix = np.load('bidafglove_tv.npy')\n",
    "        num_embeddings, embedding_dim = weights_matrix.shape\n",
    "        embedding = nn.Embedding.from_pretrained(torch.FloatTensor(weights_matrix).to(self.device),freeze=True)\n",
    "\n",
    "        return embedding\n",
    "        \n",
    "    def forward(self, ctx, ques, char_ctx, char_ques):\n",
    "        # ctx = [bs, ctx_len]\n",
    "        # ques = [bs, ques_len]\n",
    "        # char_ctx = [bs, ctx_len, ctx_word_len]\n",
    "        # char_ques = [bs, ques_len, ques_word_len]\n",
    "        \n",
    "        ctx_len = ctx.shape[1]\n",
    "        \n",
    "        ques_len = ques.shape[1]\n",
    "        \n",
    "        ## GET WORD AND CHARACTER EMBEDDINGS\n",
    "        \n",
    "        ctx_word_embed = self.word_embedding(ctx)\n",
    "        # ctx_word_embed = [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        ques_word_embed = self.word_embedding(ques)\n",
    "        # ques_word_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "        ctx_char_embed = self.character_embedding(char_ctx)\n",
    "        # ctx_char_embed =  [bs, ctx_len, emb_dim]\n",
    "        \n",
    "        ques_char_embed = self.character_embedding(char_ques)\n",
    "        # ques_char_embed = [bs, ques_len, emb_dim]\n",
    "        \n",
    "        ## CREATE CONTEXTUAL EMBEDDING\n",
    "        \n",
    "        ctx_contextual_inp = torch.cat([ctx_word_embed, ctx_char_embed],dim=2)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ques_contextual_inp = torch.cat([ques_word_embed, ques_char_embed],dim=2)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        ctx_contextual_emb = self.contextual_embedding(ctx_contextual_inp)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ques_contextual_emb = self.contextual_embedding(ques_contextual_inp)\n",
    "        # [bs, ques_len, emb_dim*2]\n",
    "        \n",
    "        \n",
    "        ## CREATE SIMILARITY MATRIX\n",
    "        \n",
    "        ctx_ = ctx_contextual_emb.unsqueeze(2).repeat(1,1,ques_len,1)\n",
    "        # [bs, ctx_len, 1, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        ques_ = ques_contextual_emb.unsqueeze(1).repeat(1,ctx_len,1,1)\n",
    "        # [bs, 1, ques_len, emb_dim*2] => [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        elementwise_prod = torch.mul(ctx_, ques_)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*2]\n",
    "        \n",
    "        alpha = torch.cat([ctx_, ques_, elementwise_prod], dim=3)\n",
    "        # [bs, ctx_len, ques_len, emb_dim*6]\n",
    "        \n",
    "        similarity_matrix = self.similarity_weight(alpha).view(-1, ctx_len, ques_len)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        \n",
    "        ## CALCULATE CONTEXT2QUERY ATTENTION\n",
    "        \n",
    "        a = F.softmax(similarity_matrix, dim=-1)\n",
    "        # [bs, ctx_len, ques_len]\n",
    "        \n",
    "        c2q = torch.bmm(a, ques_contextual_emb)\n",
    "        # [bs] ([ctx_len, ques_len] X [ques_len, emb_dim*2]) => [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        \n",
    "        ## CALCULATE QUERY2CONTEXT ATTENTION\n",
    "        \n",
    "        b = F.softmax(torch.max(similarity_matrix,2)[0], dim=-1)\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        b = b.unsqueeze(1)\n",
    "        # [bs, 1, ctx_len]\n",
    "        \n",
    "        q2c = torch.bmm(b, ctx_contextual_emb)\n",
    "        # [bs] ([bs, 1, ctx_len] X [bs, ctx_len, emb_dim*2]) => [bs, 1, emb_dim*2]\n",
    "        \n",
    "        q2c = q2c.repeat(1, ctx_len, 1)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ## QUERY AWARE REPRESENTATION\n",
    "        \n",
    "        G = torch.cat([ctx_contextual_emb, c2q, \n",
    "                       torch.mul(ctx_contextual_emb,c2q), \n",
    "                       torch.mul(ctx_contextual_emb, q2c)], dim=2)\n",
    "        \n",
    "        # [bs, ctx_len, emb_dim*8]\n",
    "        \n",
    "        \n",
    "        ## MODELING LAYER\n",
    "        \n",
    "        M, _ = self.modeling_lstm(G)\n",
    "        # [bs, ctx_len, emb_dim*2]\n",
    "        \n",
    "        ## OUTPUT LAYER\n",
    "        \n",
    "        M2, _ = self.end_lstm(M)\n",
    "        \n",
    "        # START PREDICTION\n",
    "        \n",
    "        p1 = self.output_start(torch.cat([G,M], dim=2))\n",
    "        # [bs, ctx_len, 1]\n",
    "        \n",
    "        p1 = p1.squeeze()\n",
    "        # [bs, ctx_len]\n",
    "        \n",
    "        #p1 = F.softmax(p1, dim=-1)\n",
    "        \n",
    "        # END PREDICTION\n",
    "        \n",
    "        p2 = self.output_end(torch.cat([G, M2], dim=2)).squeeze()\n",
    "        # [bs, ctx_len, 1] => [bs, ctx_len]\n",
    "        \n",
    "        #p2 = F.softmax(p2, dim=-1)\n",
    "        \n",
    "        \n",
    "        return p1, p2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    ">  *We use 100 1D ﬁlters for CNN char embedding, each with a width of 5. The hidden state size (d) of the model is 100. The model has about 2.6 million parameters. We use the AdaDelta(Zeiler,2012) optimizer, with a mini batch size of 60 and an initial learning rate of 0.5, for 12 epochs. A dropout rate of 0.2 isused for the CNN, all LSTM layers, and the linear transformation before the softmax for the answers.*\n",
    "\n",
    "__Note__- Although the mini-batch size mentioned here is 60, you might need to change this depending on your GPU. The authors have trained this model on Titan X. I have used GTX 1080 Ti for training this model which has a RAM of 11.2 GB. I had to reduce my mini-batch size to 16 or 12 to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_VOCAB_DIM = len(char2idx)\n",
    "EMB_DIM = 100\n",
    "CHAR_EMB_DIM = 8\n",
    "NUM_OUTPUT_CHANNELS = 100\n",
    "KERNEL_SIZE = (8,CHARACTER_CHANNEL_WIDTH)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "model = BiDAF(CHAR_VOCAB_DIM, \n",
    "              EMB_DIM, \n",
    "              CHAR_EMB_DIM, \n",
    "              NUM_OUTPUT_CHANNELS, \n",
    "              KERNEL_SIZE, \n",
    "              HIDDEN_SIZE, \n",
    "              device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:887\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# When constructing the global instances, we need to perform certain updates\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# by explicitly calling the superclass (dict.update, dict.items) to avoid\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# triggering resolution of _auto_backend_sentinel.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m rcParamsDefault \u001b[38;5;241m=\u001b[39m _rc_params_in_file(\n\u001b[1;32m--> 887\u001b[0m     \u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatplotlibrc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    888\u001b[0m     \u001b[38;5;66;03m# Strip leading comment.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m     transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m line: line[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m line,\n\u001b[0;32m    890\u001b[0m     fail_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(rcParamsDefault, rcsetup\u001b[38;5;241m.\u001b[39m_hardcoded_defaults)\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Normally, the default matplotlibrc file contains *no* entry for backend (the\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;66;03m# corresponding line starts with ##, not #; we fill on _auto_backend_sentinel\u001b[39;00m\n\u001b[0;32m    894\u001b[0m \u001b[38;5;66;03m# in that case.  However, packagers can set a different default backend\u001b[39;00m\n\u001b[0;32m    895\u001b[0m \u001b[38;5;66;03m# (resulting in a normal `#backend: foo` line) in which case we should *not*\u001b[39;00m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# fill in _auto_backend_sentinel.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py:559\u001b[0m, in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_data_path\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    Return the `pathlib.Path` to a resource file provided by Matplotlib.\u001b[39;00m\n\u001b[0;32m    556\u001b[0m \n\u001b[0;32m    557\u001b[0m \u001b[38;5;124;03m    ``*args`` specify a path relative to the base data path.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(\u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data_path\u001b[49m(), \u001b[38;5;241m*\u001b[39margs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "    \n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as \n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads= []\n",
    "    layers = []\n",
    "    for n, p in named_parameters:\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"c\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads)+1, lw=2, color=\"k\" )\n",
    "    plt.xticks(range(0,len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom = -0.001, top=0.02) # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"c\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"k\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPTIMIZER==\"AdaDelta\":\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER==\"Adam\":\n",
    "    optimizer=optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "elif OPTIMIZER==\"SGD\":\n",
    "    optimizer=optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9, weight_decay=5e-4) # momentum added\n",
    "else:\n",
    "    raise Exception(\"Unrecognized optimizer chosen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataset):\n",
    "    print(\"Starting training ........\")\n",
    "   \n",
    "\n",
    "    train_loss = 0.\n",
    "    batch_count = 0\n",
    "    model.train()\n",
    "    for batch in train_dataset:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch: {batch_count}\")\n",
    "        batch_count += 1\n",
    "        \n",
    "        context, question, char_ctx, char_ques, label, ctx_text, ans, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "\n",
    "\n",
    "        preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "        start_pred, end_pred = preds\n",
    "\n",
    "        s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "        loss = F.cross_entropy(start_pred, s_idx) + F.cross_entropy(end_pred, e_idx)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        #plot_grad_flow(model.named_parameters())\n",
    "        \"\"\"\n",
    "        for name, param in model.named_parameters():\n",
    "            if(param.requires_grad) and (\"bias\" not in name):\n",
    "                writer.add_histogram(name+'_grad',param.grad.abs().mean())\n",
    "        \"\"\"\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss/len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataset):\n",
    "    \n",
    "    print(\"Starting validation .........\")\n",
    "   \n",
    "    valid_loss = 0.\n",
    "\n",
    "    batch_count = 0\n",
    "    \n",
    "    f1, em = 0., 0.\n",
    "    \n",
    "    model.eval()\n",
    "        \n",
    "   \n",
    "    predictions = {}\n",
    "    \n",
    "    for batch in valid_dataset:\n",
    "\n",
    "        if batch_count % 500 == 0:\n",
    "            print(f\"Starting batch {batch_count}\")\n",
    "        batch_count += 1\n",
    "\n",
    "        context, question, char_ctx, char_ques, label, ctx, answers, ids = batch\n",
    "\n",
    "        context, question, char_ctx, char_ques, label = context.to(device), question.to(device),\\\n",
    "                                   char_ctx.to(device), char_ques.to(device), label.to(device)\n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            s_idx, e_idx = label[:,0], label[:,1]\n",
    "\n",
    "            preds = model(context, question, char_ctx, char_ques)\n",
    "\n",
    "            p1, p2 = preds\n",
    "\n",
    "            \n",
    "            loss = F.cross_entropy(p1, s_idx) + F.cross_entropy(p2, e_idx)\n",
    "\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            batch_size, c_len = p1.size()\n",
    "            ls = nn.LogSoftmax(dim=1)\n",
    "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
    "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
    "            score, s_idx = score.max(dim=1)\n",
    "            score, e_idx = score.max(dim=1)\n",
    "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
    "            \n",
    "           \n",
    "            for i in range(batch_size):\n",
    "                id = ids[i]\n",
    "                pred = context[i][s_idx[i]:e_idx[i]+1]\n",
    "                pred = ' '.join([idx2word[idx.item()] for idx in pred])\n",
    "                predictions[id] = pred\n",
    "            \n",
    "\n",
    "    \n",
    "    em, f1 = evaluate(predictions)\n",
    "    return valid_loss/len(valid_dataset), em, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions):\n",
    "    '''\n",
    "    Gets a dictionary of predictions with question_id as key\n",
    "    and prediction as value. The validation dataset has multiple \n",
    "    answers for a single question. Hence we compare our prediction\n",
    "    with all the answers and choose the one that gives us\n",
    "    the maximum metric (em or f1). \n",
    "    This method first parses the JSON file, gets all the answers\n",
    "    for a given id and then passes the list of answers and the \n",
    "    predictions to calculate em, f1.\n",
    "    \n",
    "    \n",
    "    :param dict predictions\n",
    "    Returns\n",
    "    : exact_match: 1 if the prediction and ground truth \n",
    "      match exactly, 0 otherwise.\n",
    "    : f1_score: \n",
    "    '''\n",
    "    with open('./data/squad_dev.json','r',encoding='utf-8') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    dataset = dataset['data']\n",
    "    f1 = exact_match = total = 0\n",
    "    for article in dataset:\n",
    "        for paragraph in article['paragraphs']:\n",
    "            for qa in paragraph['qas']:\n",
    "                total += 1\n",
    "                if qa['id'] not in predictions:\n",
    "                    continue\n",
    "                \n",
    "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
    "                \n",
    "                prediction = predictions[qa['id']]\n",
    "                \n",
    "                exact_match += metric_max_over_ground_truths(\n",
    "                    exact_match_score, prediction, ground_truths)\n",
    "                \n",
    "                f1 += metric_max_over_ground_truths(\n",
    "                    f1_score, prediction, ground_truths)\n",
    "                \n",
    "    \n",
    "    exact_match = 100.0 * exact_match / total\n",
    "    f1 = 100.0 * f1 / total\n",
    "    \n",
    "    return exact_match, f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_answer(s):\n",
    "    '''\n",
    "    Performs a series of cleaning steps on the ground truth and \n",
    "    predicted answer.\n",
    "    '''\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    '''\n",
    "    Returns maximum value of metrics for predicition by model against\n",
    "    multiple ground truths.\n",
    "    \n",
    "    :param func metric_fn: can be 'exact_match_score' or 'f1_score'\n",
    "    :param str prediction: predicted answer span by the model\n",
    "    :param list ground_truths: list of ground truths against which\n",
    "                               metrics are calculated. Maximum values of \n",
    "                               metrics are chosen.\n",
    "                            \n",
    "    \n",
    "    '''\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "        \n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns f1 score of two strings.\n",
    "    '''\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    '''\n",
    "    Returns exact_match_score of two strings.\n",
    "    '''\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Helper function to record epoch time.\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Starting training ........\n",
      "Starting batch: 0\n",
      "batch size: 8\n",
      "batch size: 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Got 8 and 16 (The offending index is 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [125], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 12\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m valid_loss, em, f1 \u001b[38;5;241m=\u001b[39m valid(model, valid_dataset)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_valid_loss\u001b[38;5;241m>\u001b[39mvalid_loss: \u001b[38;5;66;03m# save the best model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [121], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_dataset)\u001b[0m\n\u001b[0;32m     16\u001b[0m context, question, char_ctx, char_ques, label, ctx_text, ans, ids \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     18\u001b[0m context, question, char_ctx, char_ques, label \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mto(device), question\u001b[38;5;241m.\u001b[39mto(device),\\\n\u001b[0;32m     19\u001b[0m                            char_ctx\u001b[38;5;241m.\u001b[39mto(device), char_ques\u001b[38;5;241m.\u001b[39mto(device), label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 22\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_ques\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m start_pred, end_pred \u001b[38;5;241m=\u001b[39m preds\n\u001b[0;32m     26\u001b[0m s_idx, e_idx \u001b[38;5;241m=\u001b[39m label[:,\u001b[38;5;241m0\u001b[39m], label[:,\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [118], line 71\u001b[0m, in \u001b[0;36mBiDAF.forward\u001b[1;34m(self, ctx, ques, char_ctx, char_ques)\u001b[0m\n\u001b[0;32m     66\u001b[0m ques_char_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharacter_embedding(char_ques)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# ques_char_embed = [bs, ques_len, emb_dim]\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m## CREATE CONTEXTUAL EMBEDDING\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m ctx_contextual_inp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mctx_word_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx_char_embed\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# [bs, ctx_len, emb_dim*2]\u001b[39;00m\n\u001b[0;32m     74\u001b[0m ques_contextual_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([ques_word_embed, ques_char_embed],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Got 8 and 16 (The offending index is 0)"
     ]
    }
   ],
   "source": [
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "ems = []\n",
    "f1s = []\n",
    "epochs = 5\n",
    "best_valid_loss=99999\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_dataset)\n",
    "    valid_loss, em, f1 = valid(model, valid_dataset)\n",
    "    \n",
    "    \n",
    "    if best_valid_loss>valid_loss: # save the best model\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': valid_loss,\n",
    "                'em':em,\n",
    "                'f1':f1,\n",
    "                }, f'{model_name}.pth')\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    ems.append(em)\n",
    "    f1s.append(f1)\n",
    "\n",
    "    print(f\"Epoch train loss : {train_loss}| Time: {epoch_mins}m {epoch_secs}s\")\n",
    "    print(f\"Epoch valid loss: {valid_loss}\")\n",
    "    print(f\"Epoch EM: {em}\")\n",
    "    print(f\"Epoch F1: {f1}\")\n",
    "    print(\"====================================================================================\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss\n",
    "print(valid_loss)\n",
    "print(em)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "* Papers read/referred:\n",
    "    1. BiDAF: https://arxiv.org/abs/1611.01603\n",
    "    2. Convolutional Neural Networks for Sentence Classification: https://arxiv.org/abs/1408.5882\n",
    "    3. Highway Networks: https://arxiv.org/abs/1505.00387\n",
    "* Other helpful links:\n",
    "    1. https://nlp.seas.harvard.edu/slides/aaai16.pdf. A great resource for character embeddings. The figures in the character embedding section are taken from here.\n",
    "    2. https://towardsdatascience.com/the-definitive-guide-to-bi-directional-attention-flow-d0e96e9e666b. A great series of blogs to understand BiDAF.\n",
    "    Some of the following repos might be out of date.\n",
    "    3. https://github.com/allenai/bi-att-flow\n",
    "    4. https://github.com/galsang\n",
    "    5. https://github.com/jojonki/BiDAF/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
